        ‚Ä£ Attendance registration App:



             Satoshi                CSC3160             Wed. 8:30-9:50             TB102
            Nakamura                                    Fri. 10:30-11:50




   ‚Ä£ It's active 15 minutes before class starts until 20 minutes
        after class start time.
   ‚Ä£ It‚Äôs active on two time slots on Wednesdays
        and Fridays; any mismatched check-ins will
        be filtered out.

                                                                                     1
Satoshi Nakamura (SDS) Slide Credit to Serena Yeung BIODS220 Stanford University
          Satoshi Nakamura@ SDS, CSC5052/AIR6063                                           2025/12/1   1
        CSC3160: Fundamentals of
     Speech and Language Processing
Lecture 17
Automatic Speech
Recognition
 Satoshi Nakamura
                                          Schedule
                  2024     Date   Class No. Topics
                            24       15    Midterm Exam
                            29       16    Lecture 13 Statistical MT
                            31       17    Lecture 14 Neural MT (Encoder-Decoder)
                11          5        18    Lecture 15 Transformer
                            7        19    Lecture 16 Self Supervised Modeling
                            12       20    No class (Absent for International Conference)
                                                                                          Online
                            14       21    Lecture 17 Automatic Speech Recognition
                            19       22    Lecture 18 Text to Speech Synthesis
                            21       23    Lecture 19 Chatbot
                            26       24    Lecture 20 QA
                            28       25    Lecture 21 LLM
                12          3        26    Lecture 22 Affects
                            5        27    Summary for the Final Exam
                                           Report Review (SN: Absent for International
                            10       28
                                           Conference)
                            12       29    Final Exam
CSC3160 Satoshi Nakamura                                                            2025/12/1
                                                                                                3
                              Assignment Plan


           Assignment        Scope     Release Deadline Answer session+
                No                                         Tutorial
                 1          Class 1-7   9/26    10/10       10/17
                 2         Class 8-12   9/28    10/21      (10/22)
                 3         Class 13-21  11/7    11/26       11/28
                 4           Report     10/24    12/3       12/10




CSC3160 Satoshi Nakamura                                          2025/12/1
                                                                              4
                                      Outline
      ‚Ä¢ Recap
      ‚Ä¢ Introduction of ASR
           ‚Ä¢ Traditional ASR
                ‚Ä¢ HMM
           ‚Ä¢ DNN-based ASR
                ‚Ä¢ Hybrid DNN-HMM
                ‚Ä¢ CTC
                ‚Ä¢ Attention Enc-Dec
                ‚Ä¢ RNN-Transducer




CSC3160 Satoshi Nakamura                        2025/12/1
                                                            5
                  Examples of self-supervision in Speech

          ‚Ä¢ wav2vec 2.0
                ‚Ä¢ The task: Predict masked speech frames
                ‚Ä¢ Contrastive loss
                     ‚Ä¢ Predicted frame representations should
                       be similar to quantized input features at
                       the same frame
                     ‚Ä¢ ‚Ä¶ and different from inputs at different
                       frames




CSC3160 Satoshi Nakamura                                           2025/12/1
                                                                               6
                  Examples of self-supervision in Speech

          ‚Ä¢ HuBERT (Hidden-unit BERT)
                ‚Ä¢ Uses quantization like wav2vec 2.0, but
                  BERT-like masked prediction loss
                ‚Ä¢ Iterates quantization and re-training


                ‚Ä¢ The task: Predict masked speech frames
                ‚Ä¢ Log loss (cross-entropy)




CSC3160 Satoshi Nakamura                                    2025/12/1
                                                                        7
                                      Outline
      ‚Ä¢ Recap
      ‚Ä¢ Introduction of ASR
           ‚Ä¢ Traditional ASR
                ‚Ä¢ HMM
           ‚Ä¢ DNN-based ASR
                ‚Ä¢ Hybrid DNN-HMM
                ‚Ä¢ CTC
                ‚Ä¢ Attention Enc-Dec
                ‚Ä¢ RNN-Transducer




CSC3160 Satoshi Nakamura                        2025/12/1
                                                            8
                           Speech Recognition Task

          ‚Ä¢ Convert human speech waveform to human text.
          ‚Ä¢ Also called automatic speech recognition (ASR) or speech-to-text (STT).
          ‚Ä¢ ASR allows human to talk to machine in the most natural way.



                                             Speech
                                                              ‚ÄúGood morning‚Ä¶‚Äù
                                           Recognition




CSC3160 Satoshi Nakamura                                                        2025/12/1
                                                                                            9
                             Speech waveforms
              Waveform:




                                            ASR is a sequential pattern recognition task.
                                            Subtasks:
                             Speech         ‚Ä¢ Where speech is? (VAD)
                           Recognition
                                            ‚Ä¢ Where is word beginning?
                                              (Segmentation)
                                            ‚Ä¢ What are each word? (Classification)

                           ‚ÄúGood morning‚Äù


CSC3160 Satoshi Nakamura                                                          2025/12/1
                                                                                              10
                                     Spectrogram and phonemes
      Waveform:                                                                ‚Ä¢ Words can be divided
                                                                                 into phonemes, only
                                                                                 about 40 phonemes
                    Frequency (Hz)

                                                                                 in English.
      Spectrogram:                                                             ‚Ä¢ Different phonemes
                                                                                 have different
                                                                                 ‚Äúpatterns‚Äù
                                                                                 ‚Ä¢ Duration
                                                                                 ‚Ä¢ Spectrum
      phonemes:
                                     /g/ /uh/ /d/ /m/ /ao/ /r/ /n/ /ih/ /ng/


      IPA:                                     /…° äd Ààm…îÀên…™≈ã/

      Sentence:                                Good morning
CSC3160 Satoshi Nakamura                                                                      2025/12/1
                                                                                                          11
                     Automatic Speech Recognition
      ‚Ä¢ Problem
           ‚Ä¢ Estimate optimal word sequence given speech input

      ‚Ä¢ Obstacles
           ‚Ä¢ Spectral variation
                ‚Ä¢ Same word uttered differently by the same speaker and different speakers.
                ‚Ä¢ Different timbers
           ‚Ä¢ Temporal variation
                ‚Ä¢ Same word uttered differently by the same speaker and different speakers.
                ‚Ä¢ Longer, shorter, different rhythms,



CSC3160 Satoshi Nakamura                                                               2025/12/1
                                                                                                   12
                      Building templates for phonemes

                                             ‚Ä¢ Each phoneme has some
                                               templates.
                                             ‚Ä¢ A templates should capture
                                               the characteristics of a
                                               phoneme, including its
                                               duration and spectral
                                               distributions.
                                             ‚Ä¢ Templates should be elastic to
                                               handle variations of
                                               phonemes.
                                             ‚Ä¢ We will cover the actual
                                               techniques later.

                            templates
CSC3160 Satoshi Nakamura                                       2025/12/1
                                                                           13
                                                      Decoding
                                                            ‚Ä¢ Decoding is a search
                                                              problem: find the most likely
                                                              word sequence given the
                                                              audio signal.
                                                            ‚Ä¢ Decoder needs to
                             Decoder
                                             templates           ‚Ä¢ detect start/end times of
                                                                   potential monophones
                /g/ /uh/ /d/ /m/ /ao/ /r/ /n/ /ih/ /ng/          ‚Ä¢ match them with the templates
                                                                   to find out the best template for
                      Find the most                                each region
                      possible words


                           Good
                           morning
CSC3160 Satoshi Nakamura                                                                  2025/12/1
                                                                                                       14
                           Statistical modeling for ASR
      ‚Ä¢ Templates: acoustic model
      ‚Ä¢ How to form words from phonemes: pronunciation model
      ‚Ä¢ How to model word relationships: language model
      ‚Ä¢ Decoding: Viterbi decoding and beam search




CSC3160 Satoshi Nakamura                                  2025/12/1
                                                                      15
                                Statistical ASR




          ‚Ä¢ Acoustic Model: Probability of generative model to produce X given W
          ‚Ä¢ Language Mode: Language model probability of W
          ‚Ä¢ Find the most probable W sequence with highest probability



CSC3160 Satoshi Nakamura                                                 2025/12/1
                                                                                     16
                                Assign Labels
                           spectrogram




CSC3160 Satoshi Nakamura                        2025/12/1
                                                            17
                           Hidden Markov Model (HMM)
                             Three States (Left to Right)




                             Generate variable length spectrum
CSC3160 Satoshi Nakamura                                         2025/12/1
                                                                             18
            Acoustic model: Hidden Markov Model (HMM)
                           for phonemes                                                                                        Self repetition


      ‚Ä¢ Each state usually has a transition                                                                                                  To next

        probability to the next state, e.g. from ùëéùëú!                                                                  ùëéùëú!         ùëéùëú"
                                                                                                                                             state
                                                                                                                                                       ùëéùëú#
        to ùëéùëú"
      ‚Ä¢ There is also a self-repetition probability:                                                                            ùëù ùëÇ$ ùëéùëú"
                                                                                                                    ùëù ùëÇ$ ùëéùëú!                           ùëù ùëÇ$ ùëéùëú#
        allow each state to ‚Äúconsume‚Äù variable
        number of frames √† handling duration
        variations of phonemes.
      ‚Ä¢ The distribution of the acoustic features at
        each state is assumed to be stationary,
        usually represented by a Gaussian mixture
        model (GMM):
                 ùëù ùëÇ# ùëÜ# = ùëéùëú! = ùê∫ùëÄùëÄ ùëéùëú!
        where ùëÇ# and ùëÜ# is the observed feature
      vector and hidden state at time index t,
      respectively.
     Rabiner, L. and Juang, B., 1986. An introduction to hidden Markov models. IEEE ASSP magazine, 3(1), pp.4-16.

CSC3160 Satoshi Nakamura                                                                                                                                     2025/12/1   19
         Likelihood (Probability Density Distribution)




                           Generate variable spectrum given the distribution
CSC3160 Satoshi Nakamura                                                  2025/12/1
                                                                                      20
             Observation distribution for HMM states:
                Gaussian Mixture Model (GMM)
                                                   Example of 1-dimensional GMM
          ‚Ä¢ GMM is a weighted sum of multiple      with 3 components
            Gaussian distributions.
                                )
                     ùëù ùëÇ% Œõ = % ùë§& ùëÅ(ùúá& , Œ£& )
                               &'(
                where ùúá! and Œ£! are mean and
                covariance of the mth Gaussian
                component.
          ‚Ä¢ GMM can model any distribution given
            enough components.
          ‚Ä¢ Number of components for each state
            varies depending on the amount of
            training data.
CSC3160 Satoshi Nakamura                                              2025/12/1
                                                                                  21
       Acoustic features: Mel-frequency cepstral
                  coefficients (MFCC)                                                               Illustration of Mel frequency filterbanks




    ‚Ä¢ Features for acoustic modelling should
           ‚Ä¢ be compact for HMM-GMM modelling;
           ‚Ä¢ carry useful information to discriminate phoneme classes.
    ‚Ä¢ MFCC is the most popular features for HMM-GMM
      acoustic model
    ‚Ä¢ Motivated by human auditory system.
           ‚Ä¢ Mel-scale filterbanks: higher frequency resolution for low
             frequencies
           ‚Ä¢ Take logarithm of speech power: mimic equal loudness
             curve and convert sound intensity to perceived loudness



   Davis, S. and Mermelstein, P. (1980) Comparison of parametric representations for monosyllabic
   word recognition in continuously spoken sentences. IEEE Transactions on Acoustics, Speech and
   Signal Processing, 28, 357-366.

CSC3160 Satoshi Nakamura                                                                                                                        2025/12/1
                                                                                                                                                            22
                                           Layers of models
                                     Hello word                Search space is usually all possible
        Language                     Good afternoon            sentences in a language
          model                      Good morning
                                     ‚Ä¶

                                                                             Pronunciati      good: g uh d
                                                                              on model        morning: m ao r n ih ng



                           /g/ /uh/ /d/ /m/ /ao/ /r/ /n/ /ih/ /ng/
                                                                                                         Acoustic
                                                                                                          model

                                                                                                      Each phoneme is
                                                                                                      represented by a three-
                                                                                                      state hidden Markov model

                                                                                                        Feature vector
                                                                                                        sequence
CSC3160 Satoshi Nakamura                                                                                       2025/12/1
                                                                                                                           23
                           A more formal view of ASR
                                                        Probability of
                                                        sentences

                                                            Language
                                                              model



                Speech          Feature                      Viterbi                       Recognized
                waveform       Extraction                   Decoder                        words
                              Extracting
                              useful
                              information


                                             Acoustic                     Pronunciati
                                              model                        on model
                                            Probabilistic                Word to phoneme
                                            templates of                 mapping
                                            phonemes

CSC3160 Satoshi Nakamura                                                                                2025/12/1
                                                                                                                    24
                           Statistical ASR




CSC3160 Satoshi Nakamura                     2025/12/1
                                                         25
                           Other choices for subword units
      ‚Ä¢ Besides phonemes and context dependent phonemes, we
        can also use other subword units for acoustic modeling.
      ‚Ä¢ Graphemes: letters or combination of letters.
      ‚Ä¢ Morphemes: smallest meaning component of words
      ‚Ä¢ BPE: variable-length units that are learned from the data.
        More popular with End-to-end ASR. Handle OOV words better.




CSC3160 Satoshi Nakamura                                     2025/12/1
                                                                         26
                           Training of the models
      ‚Ä¢ HMM/GMM acoustic model: EM (expectation-maximization)
        algorithm. Forward/backward algorithm for efficiency
      ‚Ä¢ Pronunciation model: usually defined by linguistics manually,
        expensive.
      ‚Ä¢ Language model: mainly counting the ngrams and smoothing




CSC3160 Satoshi Nakamura                                        2025/12/1
                                                                            27
                                  Evaluation Metrics
      ‚Ä¢ Reference: The quick brown fox jumped over the lazy dog
      ‚Ä¢ Hypothesis: The quick brown fox jumps over ---- lazy dog too


      ‚Ä¢ Word error rate:
                   !"#"$
           ‚Ä¢ ùëäùê∏ùëÖ =
                     %
           ‚Ä¢ D: number of deleted words
           ‚Ä¢ S: number of subsituted words
           ‚Ä¢ I: number of inserted words
           ‚Ä¢ N: number of reference words


      ‚Ä¢ Readability: whether the recognized text is easy to read by human.


CSC3160 Satoshi Nakamura                                                     2025/12/1
                                                                                         28
                Deep learning




CSC3160 Satoshi Nakamura        2025/12/1
                                            29
                           Deep learning for ASR
      ‚Ä¢ Hybrid system: only replace HMM/GMM acoustic model with
        neural networks
      ‚Ä¢ End-to-end ASR: replace the whole ASR system with neural
        works




CSC3160 Satoshi Nakamura                                     2025/12/1
                                                                         30
                              Hybrid acoustic model
          ‚Ä¢ Replace the generative
            HMM/GMM with a discriminative                   Frame level
            neural networks                                class (senone)
                                                                label
          ‚Ä¢ HMM/GMM models ùëù ùëú% ùë†%
          ‚Ä¢ Hybrid models ùëù ùë†% ùëú%
          ‚Ä¢ Common practices                                 Encoder
                ‚Ä¢ Train an HMM/GMM first
                ‚Ä¢ Use it to align the label (senone
                                                             Acoustic
                  sequences) to the feature sequence.
                                                             features
                ‚Ä¢ Train neural networks to predict frame
                  level senone labels


CSC3160 Satoshi Nakamura                                                    2025/12/1
                                                                                        31
                           Encoder Structures
      ‚Ä¢ DNN
      ‚Ä¢ CNN
      ‚Ä¢ LSTM
      ‚Ä¢ Transformer
      ‚Ä¢ Or any combination of them




CSC3160 Satoshi Nakamura                        2025/12/1
                                                            32
                     End-to-end ASR


          ‚Ä¢ End-to-end ASR systems try to
            do ASR with a single model
          ‚Ä¢ Three main approaches
                ‚Ä¢ Connectionist Temporal
                  Classification
                ‚Ä¢ RNN Transducers
                ‚Ä¢ Sequence-to-Sequence
                               Figure adapted from [2111.01690] Recent
                               Advances in End-to-End Automatic Speech
                               Recognition (arxiv.org)

CSC3160 Satoshi Nakamura                                                 2025/12/1
                                                                                     33
             Connectionist Temporal Classification (CTC)

   ‚Ä¢ Produce a label each frame. As
     feature frames is usually much
     longer than label sequence, CTC
     introduces a blank label ‚ÄùB‚Äù
     between two labels.
   ‚Ä¢ Collapse: Merge the same labels
     and remove blank labels ‚ÄúB‚Äù.
   ‚Ä¢ Strength: No frame level
     alignment required, sum over all                                               Figure adapted from ‚ÄúTara Sainath - End-to-End Speech
                                                                                    Recognition: The Journey from Research to Production ‚Äì YouTube‚Äù
     possible alignments
            Graves, A. and Jaitly, N., 2014, June. Towards end-to-end speech recognition with recurrent
            neural networks. In International conference on machine learning (pp. 1764-1772). PMLR.


CSC3160 Satoshi Nakamura                                                                                                             2025/12/1
                                                                                                                                                      34
               Connectionist Temporal Classification
‚Ä¢ Method for labeling unsegmented data sequences
‚Ä¢ Sum up all possible label sequences to calculate p(y|x)           Remove
                                                            Merge




                            [Graves et. al 2006]

 CSC3160 Satoshi Nakamura

                                                                       2025/12/1   35
                      Connectionist Temporal Classification
          ‚Ä¢ CTC output sequenceÔºö
                    ≈∂ = ( _, y1, _, y2, ..., yL, _ ), X=(x1, ‚Ä¶ xt‚Ä¶xT)
          ‚Ä¢ Sum up all possible label sequence probabilities or
          ‚Ä¢ Forward Probability :
             Œ±_t(s) = y_t(l_s) √ó [Œ±_{t‚àí1}(s) + Œ±_{t‚àí1}(s‚àí1)
                                  + ùüô(l_s ‚â† l_{s‚àí2}) ¬∑ Œ±_{t‚àí1}(s‚àí2)].
               ‚Ä¢ don‚Äôt count up if the labels are the same.

          ‚Ä¢ Probability : P(Y|X) = Œ±(T, |≈∂|‚àí1) + Œ±(T, |≈∂|)
          ‚Ä¢ Read appendix !

                                                                        2025/12/1   36
CSC3160 Satoshi Nakamura
                               Sequence-to-sequence (S2S)

          ‚Ä¢ S2S is also called attention
            encoder decoder (AED)
          ‚Ä¢ Encoder: similar to acoustic
            model
          ‚Ä¢ Attention: alignment model
          ‚Ä¢ Decoder: similar to pronunciation
            and language model
          ‚Ä¢ Offline model
          Chan, W., Jaitly, N., Le, Q.V. and Vinyals, O., 2015. Listen, attend and spell. arXiv
          preprint arXiv:1508.01211.                                                              Figure adapted from [2111.01690] Recent
          Chorowski, J.K., Bahdanau, D., Serdyuk, D., Cho, K. and Bengio, Y., 2015. Attention-    Advances in End-to-End Automatic Speech
          based models for speech recognition. Advances in neural information processing          Recognition (arxiv.org)
          systems, 28.
CSC3160 Satoshi Nakamura                                                                                                          2025/12/1
                                                                                                                                              37
                                   RNN Transducers (RNN-T)


          ‚Ä¢ Called RNN-T because originally
            RNN is used as the encoder
            model structure.
          ‚Ä¢ Newer models uses transformers
            or conformers as encoder
          ‚Ä¢ A native streaming model
          Graves, A., 2012. Sequence transduction with recurrent neural
          networks. arXiv preprint arXiv:1211.3711.
                                                                          Figure adapted from [2111.01690] Recent
                                                                          Advances in End-to-End Automatic Speech
                                                                          Recognition (arxiv.org)

CSC3160 Satoshi Nakamura                                                                                2025/12/1
                                                                                                                    38
 Open AI
 Whisper




 https://github.com/
 openai/whisper

CSC3160 Satoshi Nakamura   2025/12/1
                                       39
                                 Challenges of ASR
      ‚Ä¢ Robustness against various factors:
           ‚Ä¢ Acoustic environment: noise, reverb, transmission channel
           ‚Ä¢ Speaker variation
           ‚Ä¢ Accent
           ‚Ä¢ Domain
      ‚Ä¢ Pronunciation variation, spontaneous speech, accent
      ‚Ä¢ Overlapped speech
      ‚Ä¢ Out of vocabulary (OOV) words, long tail effect
      ‚Ä¢ Multilingual/Code-switching
      ‚Ä¢ Low-resource languages
      ‚Ä¢ Fast adaptation

CSC3160 Satoshi Nakamura                                                 2025/12/1
                                                                                     40
                           More resources on E2E ASR
      ‚Ä¢ Review papers
           ‚Ä¢ [2111.01690] Recent Advances in End-to-End Automatic Speech
             Recognition (arxiv.org)

      ‚Ä¢ Videos
           ‚Ä¢ Dr. Jinyu Li, Microsoft, "Recent Advances in End-to-End Automatic
             Speech Recognition" - CSIP Seminar ‚Äì YouTube
           ‚Ä¢ SANE2022 | Tara Sainath - End-to-End Speech Recognition: The
             Journey from Research to Production ‚Äì YouTube




CSC3160 Satoshi Nakamura                                                  2025/12/1
                                                                                      41
                                                           Readings
      ‚Ä¢ Ortmanns and Ney (2000). \The time-conditioned approach in dynamic programming search for LVCSR".
        In IEEE Transactions on Speech and Audio Processing
      ‚Ä¢ Peter Bell, ‚ÄúASR Lecture‚Äù, University of Edinburgh
        https://www.inf.ed.ac.uk/teaching/courses/asr/index-2025.html
        https://www.inf.ed.ac.uk/teaching/courses/asr/index-2023.html
      ‚Ä¢ G&Y: MJF Gales and SJ Young (2007). The Application of Hidden Markov Models in Speech Recognition, Foundations and Trends in
        Signal Processing, 1 (3), 195-304.
      ‚Ä¢ S Young (1996). A review of large-vocabulary continuous-speech recognition, IEEE Signal Processing Magazine 13 (5), 45-57.
      ‚Ä¢ R&H:S Renals and T Hain (2010). Speech Recognition, in Computational Linguistics and Natural Language Processing Handbook, A
        Clark, C Fox and S Lappin (eds.), Blackwells, chapter 12, 299-332.
      ‚Ä¢ G Hinton et al (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, IEEE
        Signal Processing Magazine, 29(6):82-97.
      ‚Ä¢ S Young (2008). HMMs and Related Speech Recognition Technologies, in Springer Handbook of Speech Processing, J Benesty, MM
        Sondhi and Y Huang (eds), chapter 27, 539-557.
      ‚Ä¢ Bell et al (2021). Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview , IEEE Open Journal of Signal
        Processing, Vol 2:33-36.



CSC3160 Satoshi Nakamura                                                                                                        2025/12/1
                                                                                                                                                  42
                                                    Readings
      ‚Ä¢ A Maas et al (2017). \Building DNN acoustic models for large vocabulary speech recognition", Computer Speech and
        Language, 41:195{213. https://arxiv.org/abs/1406.7806

      ‚Ä¢ V Peddinti et al (2015). \A time delay neural network architecture for e cient modeling of long temporal contexts",
        Interspeech. https://www.isca-speech.org/archive/interspeech_2015/i15_3214.html

      ‚Ä¢ Background Reading:

      ‚Ä¢ G Hinton et al (Nov 2012). \Deep neural networks for acoustic modeling in speech recognition", IEEE Signal

      ‚Ä¢ Processing Magazine, 29(6), 82{97. http://ieeexplore.ieee.org/document/6296526

      ‚Ä¢ Herv e Bourlard (1992). \CDNN: A context-dependent neural network for speech recognition", Proc. ICASSP

      ‚Ä¢ ASR Lecture 12 NNs for Acoustic Modelling 3: CD DNNs, TDNNs and LSTMs20

      ‚Ä¢ Open AI Whisper: https://github.com/openai/whisper




CSC3160 Satoshi Nakamura                                                                                        2025/12/1
                                                                                                                              43
                            Appendix:
                    CTC Forward Probability (T=3)
                             Example




CSC3160 Satoshi Nakamura                            2025/12/1
                                                                44
                                 Setup and Notation
      Given target label Y = (A), compute P(Y|X) using the forward algorithm
      (T=3).


      ‚Ä¢ Steps
           ‚Ä¢ Extended label sequence L' = [_, A, _] (S = 3).
           ‚Ä¢ Frame-wise posteriors:
           ‚Ä¢ t=1: y1(A)=0.2, y1(_)=0.8
           ‚Ä¢ t=2: y2(A)=0.3, y2(_)=0.7
           ‚Ä¢ t=3: y3(A)=0.7, y3(_)=0.3




CSC3160 Satoshi Nakamura                                               2025/12/1
                                                                                   45
                           Definition of Forward Probability
      ‚Ä¢ Definition and recurrence of Œ±_t(s)?
      ‚Ä¢ Steps:
           ‚Ä¢ Œ±_t(s): probability of reaching output position s at time t.
           ‚Ä¢ Initialization: Œ±1(1)=y1(_), Œ±1(2)=y1(A), Œ±1(3)=0.
           ‚Ä¢ Recurrence: Œ±_t(s) = y_t(l_s) √ó [Œ±_{t‚àí1}(s) + Œ±_{t‚àí1}(s‚àí1) + ùüô(l_s ‚â† l_{s‚àí2})¬∑Œ±_{t‚àí1}(s‚àí2)].
           ‚Ä¢ For L'=[_,A,_], l3=_ and l1=_, so ùüô(l3‚â†l1)=0 (no s‚àí2 term when s=3).
           ‚Ä¢ Final probability: P(Y|X) = Œ±_T(S) + Œ±_T(S‚àí1).




CSC3160 Satoshi Nakamura                                                                       2025/12/1
                                                                                                            46
                            Step 1: Initialization (t=1)
      ‚Ä¢ Compute Œ±1(s) for all positions in L‚Äô.
      ‚Ä¢ Steps:
           ‚Ä¢ Œ±1(1) = y1(_) = 0.8
           ‚Ä¢ Œ±1(2) = y1(A) = 0.2
           ‚Ä¢ Œ±1(3) = 0




CSC3160 Satoshi Nakamura                                   2025/12/1
                                                                       47
                           Step 2: Recurrence at t=2
      ‚Ä¢ Compute Œ±2(s) using Œ±1(s).
      ‚Ä¢ Steps:
           ‚Ä¢ Œ±2(1) = y2(_)*Œ±1(1) = 0.7√ó0.8 = 0.56
           ‚Ä¢ Œ±2(2) = y2(A)*(Œ±1(2)+Œ±1(1)) = 0.3√ó(0.2+0.8) = 0.3
           ‚Ä¢ Œ±2(3) = y2(_)*(Œ±1(3)+Œ±1(2)) = 0.7√ó(0+0.2) = 0.14




CSC3160 Satoshi Nakamura                                         2025/12/1
                                                                             48
                           Step 3: Recurrence at t=3
      ‚Ä¢ Question: Compute Œ±3(s) using Œ±2(s).
      ‚Ä¢ Steps:
           ‚Ä¢ Œ±3(1) = y3(_)*Œ±2(1) = 0.3√ó0.56 = 0.168
           ‚Ä¢ Œ±3(2) = y3(A)*(Œ±2(2)+Œ±2(1)) = 0.7√ó(0.3+0.56) = 0.602
           ‚Ä¢ Œ±3(3) = y3(_)*(Œ±2(3)+Œ±2(2)) = 0.3√ó(0.14+0.3) = 0.132




CSC3160 Satoshi Nakamura                                            2025/12/1
                                                                                49
                             Step 4: Final Probability
      ‚Ä¢ Question: Compute P(Y|X).
      ‚Ä¢ Steps:
           ‚Ä¢ P(Y|X) = Œ±3(3) + Œ±3(2) = 0.132 + 0.602 = 0.734
           ‚Ä¢ This is the total probability of generating Y=(A) given X.




CSC3160 Satoshi Nakamura                                                  2025/12/1
                                                                                      50
          Step 5: Cross-check via Path Enumeration
      ‚Ä¢ Question: Verify result by summing probabilities of all valid alignment
        paths.
      ‚Ä¢ Steps:
           ‚Ä¢ Valid paths: AAA, AA_, A__, _AA, _A_, __A.
           ‚Ä¢ Each path probability:
           ‚Ä¢ AAA: 0.2√ó0.3√ó0.7 = 0.042
           ‚Ä¢ AA_: 0.2√ó0.3√ó0.3 = 0.018
           ‚Ä¢ A__: 0.2√ó0.7√ó0.3 = 0.042
           ‚Ä¢ _AA: 0.8√ó0.3√ó0.7 = 0.168
           ‚Ä¢ _A_: 0.8√ó0.3√ó0.3 = 0.072
           ‚Ä¢ __A: 0.8√ó0.7√ó0.7 = 0.392
           ‚Ä¢ Sum = 0.734 (matches forward probability).

CSC3160 Satoshi Nakamura                                                   2025/12/1
                                                                                       51
