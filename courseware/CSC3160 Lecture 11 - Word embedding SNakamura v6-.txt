   ‚Ä£ Attendance registration App:



     Satoshi  CSC3160 Wed. 8:30-9:50                  TB102
     Nakamura         Fri. 10:30-11:50




 ‚Ä¢ It's active 15 minutes before class starts until 20 minutes
   after class start time.

 ‚Ä¢ It‚Äôs active on two time slots on Wednesdays
   and Fridays; any mismatched check-ins will be
   filtered out.

Satoshi Nakamura, CUHKSZ CSC3160
                                                                 1
CSC3160 - Fundamentals of Speech and Language
Processing



                                Lecture 11
                        Embedding: Representations of
                           the meaning of words
                                   Satoshi Nakamura


                                          2
Satoshi Nakamura, CUHKSZ CSC3160                      10/22/25   2
                                            Schedule
                  2025       Date    Class No. Topics
                     9        3          1     Lecture 1 Introduction
                              5          2     Lecture 2 Sound and acoustics
                                               Lecture 3 Lecture 3 Understanding human speech
                               10        3
                                               production
                               12        4     Lecture 4 Speech Perception
                               17        5     Lecture 5 Digital Speech Processing
                               19        6     Lecture 6 Speech representation
                               24        7     Lecture 7 Phonetics
                               26        8     Lecture 8 Text processing (No speech coding )
         (make-up)             28        9     Lecture 9 Words
                 10            (1)   Holiday
                               (3)   Holiday
                               (8)   Holiday
                               10       10     Lecture 10 Syntax
       (make-up?)              11       11     No class
                               15       12     Lecture 11 Word embeddings
                               17       13     Lecture 12 Language models
                               22       14     Midterm Summary

Satoshi Nakamura, CUHKSZ CSC3160
                                                                                                3
                                            Schedule
                                                                              Attention:
                                                                          Schedule changed
                 2024        Date   Class No. Topics
                               24      15    Midterm Exam
                               29      16    Lecture 13 Statistical MT
                               31      17    Lecture 14 Neural MT (Encoder-Decoder)
              11                5      18    Lecture 15 Transformer
                                7      19    Lecture 16 Self Supervised Modeling
                               12      20    No class (Absent for International Conference)
                               14      21    Lecture 17 Automatic Speech Recognition
                               19      22    Lecture 18 Text to Speech Synthesis
                               21      23    Lecture 19 Chatbot
                               26      24    Lecture 20 QA
                               28      25    Lecture 21 LLM
              12                3      26    Lecture 22 Affects
                                5      27    Summary for the Final Exam
                                             Report Review (SN: Absent for International
                               10      28
                                             Conference)
                               12      29    Final Exam
Satoshi Nakamura, CUHKSZ CSC3160
                                                                                              4
                                      Assignment Plan


         Assignment                  Scope     Release Deadline Answer session+
              No                                                   Tutorial
               1                    Class 1-7    9/26   10/10       10/17
               2                   Class 8-12    9/28   10/21      (10/22)
               3                   Class 16-22   11/1   11/26       11/28
               4                     Report     10/24    12/3       12/10




Satoshi Nakamura, CUHKSZ CSC3160
                                                                                  5
                                   Midterm Exam
   ‚Ä¢ Mid-term exam: October 24 10:30-12:30
                              th

         ‚Ä¢ Scope: Lecture 1- 13 (all lectures until the midterm exam)

         ‚Ä¢ Condition
              ‚Ä¢ No mobile phone, Pad, or PC.
              ‚Ä¢ 1-page both-side cheat sheet
              ‚Ä¢ no magnifying glass




Satoshi Nakamura, CUHKSZ CSC3160
                                                                        6
                                   Hands-on and Tutorial
   ‚Ä¢ Requests for Hands-on
         ‚Ä¢ hands-on will be released on BB

   ‚Ä¢ Tutorials
         ‚Ä¢ We have a tutorial this Friday.




Satoshi Nakamura, CUHKSZ CSC3160
                                                           7
                                   Agenda
     ‚Ä¢ Recap
     ‚Ä¢ Word sense and their relations
     ‚Ä¢ Word representation and embedding
     ‚Ä¢ Measuring semantic similarity




Satoshi Nakamura, CUHKSZ CSC3160            10/22/25   8
                                   Constituency
     ‚Ä¢ Constituent: a group of neighboring words that relate more
       closely to one another than to other words in the sentence
     ‚Ä¢ Constituents larger than a word are called phrases
          ‚Ä¢ Noun phrases
          ‚Ä¢ Prepositional phrases
          ‚Ä¢ Verb phrases
     ‚Ä¢ Phrases can contain other phrases
     ‚Ä¢ One way of viewing the structure of a sentence is as a
       collection of nested constituents

Satoshi Nakamura, CUHKSZ CSC3160
                                                                    9
                                   Chomsky hierarchy
  ‚Ä¢ Type-0 grammars include all formal
    grammars
  ‚Ä¢ Type-1 grammars generate context-
    sensitive languages                        Type-0

  ‚Ä¢ Type-2 grammars generate the
                                                 Type-1
    context-free languages
  ‚Ä¢ Type-3 grammars generate the                    Type-2
    regular languages, which can be
                                                        Type-3
    described using regular expressions


Satoshi Nakamura, CUHKSZ CSC3160
                                                                 10
                                   Rules or productions

‚Ä¢ Context-free
      ‚Ä¢ production rules are
        independent of the context


      ‚Ä¢ There is no context in the left
        hand side (LHS) of rules




Satoshi Nakamura, CUHKSZ CSC3160
                                                          11
                              A (Constituency) Parse Tree




Satoshi Nakamura, CUHKSZ CSC3160
                                                            12
                                   Chomsky Normal Form




Satoshi Nakamura, CUHKSZ CSC3160
                                                         13
Satoshi Nakamura, CUHKSZ CSC3160
                                   14
                                            CKY algorithm




                                   https://web.stanford.edu/~jurafsky/slp3/17.pdf

Satoshi Nakamura, CUHKSZ CSC3160
                                                                                    15
                                    Ambiguity
‚Ä¢ Structural ambiguity occurs when the grammar can assign more
  than one parse to a sentence




    One morning I shot an elephant in my pajamas




 Satoshi Nakamura, CUHKSZ CSC3160
                                                                 16
                                   Agenda
     ‚Ä¢ Recap
     ‚Ä¢ Word sense and their relations
     ‚Ä¢ Word representation and embedding
     ‚Ä¢ Measuring semantic similarity




Satoshi Nakamura, CUHKSZ CSC3160            10/22/25   17
        Bank




Satoshi Nakamura, CUHKSZ CSC3160   10/22/25   18
        Bank




Satoshi Nakamura, CUHKSZ CSC3160   10/22/25   19
                                   Bank




Satoshi Nakamura, CUHKSZ CSC3160          10/22/25   20
                                        Word sense
       ‚Ä¢ Word sense vs Lemma

                        Lemma

               Play (N)
                ‚Ä¢ a theatrical performance of a drama, "the play lasted two hours"
Sense           ‚Ä¢ a preset plan of action in team sports, "the coach drew up the plays for
                  her team"
                ‚Ä¢ a state in which action is feasible, "the ball was still in play"; "insiders said
                  the company's stock was in play"
                ‚Ä¢ utilization or exercise, "the play of the imagination"


Satoshi Nakamura, CUHKSZ CSC3160                                          10/22/25             21
           Distributional Hypothesis (Lenci, 2008)


  ‚Ä¢ At least certain aspects of the meaning of lexical expressions
   depend on their distributional properties in the linguistic contexts


  ‚Ä¢ The degree of semantic similarity between two linguistic
   expressions is a function of the similarity of the two linguistic
   contexts in which they can appear


Satoshi Nakamura@SDS AIR6001/MDS6105/CSC5010                              22
                                   Word sense (concept)

     ‚Ä¢ He wrote several plays but only one was produced on Broadway
     ‚Ä¢ Insiders said the company's stock was in play
     ‚Ä¢ The runner was out on a play by the shortstop




Satoshi Nakamura, CUHKSZ CSC3160                          10/22/25   23
                 Relations between senses: Synonymy
     ‚Ä¢ Synonyms have the same meaning in some or all contexts
          ‚Ä¢ couch/sofa
          ‚Ä¢ large/big
          ‚Ä¢ water/H2O




Satoshi Nakamura, CUHKSZ CSC3160               10/22/25         24
                  Relations between senses: Similarity
     ‚Ä¢ Words with similar meanings
     ‚Ä¢ Not synonyms, but sharing some element of meaning
          ‚Ä¢ Car, bicycle
          ‚Ä¢ Cow, horse




Satoshi Nakamura, CUHKSZ CSC3160                   10/22/25   25
             Relations between senses: Relatedness
     ‚Ä¢ Also named as word association
     ‚Ä¢ Words can be related in any way, perhaps via a semantic
       frame or field


          ‚Ä¢ Similar: coffee, tea
          ‚Ä¢ Related (but not similar)
               ‚Ä¢ coffee, cup




Satoshi Nakamura, CUHKSZ CSC3160                10/22/25         26
                 Relations between senses: Antonymy
     ‚Ä¢ Senses that are opposites with respect to only one
       feature of meaning


          ‚Ä¢ Examples
               ‚Ä¢ Short/long
               ‚Ä¢ Hot/cold
               ‚Ä¢ In/out



Satoshi Nakamura, CUHKSZ CSC3160              10/22/25      27
              Relations between senses: Connotation
     ‚Ä¢ Affective meaning of words
          ‚Ä¢ fake, knockoff, forgery
          ‚Ä¢ copy, replica, reproduction




Satoshi Nakamura, CUHKSZ CSC3160           10/22/25   28
                                   Cross lingual
                    ‚Ä¢ Banana
                    ‚Ä¢ È¶ôËïâ
                    ‚Ä¢ „Éê„Éä„Éä
                    ‚Ä¢ Î∞îÎÇòÎÇò
                    ‚Ä¢ pl√°tano
                    ‚Ä¢ qu·∫£ chu·ªëi



Satoshi Nakamura, CUHKSZ CSC3160                   10/22/25   29
                         Cross-lingual word embedding




Satoshi Nakamura, CUHKSZ CSC3160                 10/22/25   30
                                   Word representation
     ‚Ä¢ Five-word vocabulary: man, walk, woman, swim, ask
          ‚Ä¢ 1-of-N encoding/one-hot encoding

               ‚Ä¢ [1, 0, 0, 0, 0]: man
               ‚Ä¢ [0, 1, 0, 0, 0]: walk
               ‚Ä¢ [0, 0, 1, 0, 0]: woman
               ‚Ä¢ [0, 0, 0, 1, 0]: swim
               ‚Ä¢ [0, 0, 0, 0, 1]: ask


Satoshi Nakamura, CUHKSZ CSC3160                         10/22/25   31
                    Word (Embedding) representations



        [1, 0, 0, 0, 0]: man
        [0, 1, 0, 0, 0]: walk
        [0, 0, 1, 0, 0]: woman
        [0, 0, 0, 1, 0]: swim
        [0, 0, 0, 0, 1]: ask




Satoshi Nakamura, CUHKSZ CSC3160             10/22/25   32
                                   Co-occurrence matrix
     ‚Ä¢ Term-document matrix
          ‚Ä¢ Each row represents a word in the vocabulary
          ‚Ä¢ Each column represents a document from some collection of
            documents


     ‚Ä¢ Term-term matrix
          ‚Ä¢ The columns are labeled by words rather than documents



Satoshi Nakamura, CUHKSZ CSC3160                       10/22/25      33
           Words as vectors: Document dimensions




                      Similar words have similar vectors
                       because they tend to occur in similar documents

Satoshi Nakamura, CUHKSZ CSC3160                         10/22/25        34
                                    Term-document matrix
     ‚Ä¢ Originally defined as a means of finding similar documents




                                   Similar documents had similar vectors

Satoshi Nakamura, CUHKSZ CSC3160                                  10/22/25   35
                                   Spatial visualization




Satoshi Nakamura, CUHKSZ CSC3160                           10/22/25   36
                                   Term-term matrix
     ‚Ä¢ The columns are labeled by words rather than documents
     ‚Ä¢ Two words are similar in meaning if their context vectors are
       similar




Satoshi Nakamura, CUHKSZ CSC3160                        10/22/25       37
                   Words as vectors: Word dimensions
     ‚Ä¢ word-word co-occurrence matrix




Satoshi Nakamura, CUHKSZ CSC3160             10/22/25   38
                                   Spatial visualization




Satoshi Nakamura, CUHKSZ CSC3160                           10/22/25   39
       Is the raw frequency a good representation?
     ‚Ä¢ Motivation
          ‚Ä¢ Frequency is clearly useful
          ‚Ä¢ However, overly frequent words like

                                   the, and, it

             are not very informative about the context


                                        We need to balance

Satoshi Nakamura, CUHKSZ CSC3160                             10/22/25   40
                                               TF-IDF
     ‚Ä¢ Term frequency (t: term id, d: document id)




                                   Instead of using raw count, we take log:




Satoshi Nakamura, CUHKSZ CSC3160                                  10/22/25    41
                                    TF-IDF
     ‚Ä¢ DF: Document Frequency
          ‚Ä¢ df is a term t that the number of documents it occurs in




Satoshi Nakamura, CUHKSZ CSC3160                          10/22/25     42
                                   TF-IDF
     ‚Ä¢ Inverse document frequency




    N is the total number of documents.




Satoshi Nakamura, CUHKSZ CSC3160            10/22/25   43
                                   TF-IDF




Satoshi Nakamura, CUHKSZ CSC3160            10/22/25   44
                      Semantic similarity measurement



  Words in the semantic
  space




Satoshi Nakamura, CUHKSZ CSC3160               10/22/25   45
                                   Inner/dot product
     ‚Ä¢ The dot product between two vectors is a scalar




           The dot product tends to be high
           when the two vectors have large values in the same dimensions


Satoshi Nakamura, CUHKSZ CSC3160                       10/22/25        46
                                   Dot-product: problem
     ‚Ä¢ Dot-product favors long vectors (i.e. vectors with larger norm)




Satoshi Nakamura, CUHKSZ CSC3160                          10/22/25       47
                                   Cosine similarity




Satoshi Nakamura, CUHKSZ CSC3160                       10/22/25   48
                         Cosine similarity: Interpretation



       ‚Ä¢ +1: same direction
       ‚Ä¢ 0: orthogonal
       ‚Ä¢ -1: opposite directions




Satoshi Nakamura, CUHKSZ CSC3160                     10/22/25   49
                                   5
Satoshi Nakamura, CUHKSZ CSC3160   0
                                       10/22/25   50
                    Pointwise Mutual Information (PMI)
      ‚Ä¢ Do events x and y co-occur more than if they were independent?




      PMI between two words
            ‚Ä¢ Do words x and y co-occur more than if they were independent?




                                   Section 6.6: https://web.stanford.edu/~jurafsky/slp3/6.pdf
                                                                                                           51
Satoshi Nakamura, CUHKSZ CSC3160                                                                10/22/25   51
                      Embedding: short, dense vector




Satoshi Nakamura, CUHKSZ CSC3160              10/22/25   52
                               Sparse versus dense vectors
     TF-IDF (or PMI) vectors are
        ‚Ä¢ long (length |V|= 20,000 to 50,000)
        ‚Ä¢ sparse (most elements are zero)
     Alternative: learn vectors which are
        ‚Ä¢ short (length 50-1000)
        ‚Ä¢ dense (most elements are non-zero)




Satoshi Nakamura, CUHKSZ CSC3160                       10/22/25   53
                               Sparse versus dense vectors
     ‚Ä¢ Why dense vectors?
          ‚Ä¢ Short vectors may be easier to use as features in machine learning
            (fewer weights to tune)
          ‚Ä¢ Dense vectors may generalize better than explicit counts
          ‚Ä¢ Dense vectors may do better at capturing synonymy:
               ‚Ä¢ car and automobile are synonyms; but are distinct dimensions
               ‚Ä¢ a word with car as a neighbor and a word with automobile as a neighbor
                 should be similar, but aren't

     ‚Ä¢ In practice, they work better


Satoshi Nakamura, CUHKSZ CSC3160                                   10/22/25          54
         Static embedding: one fixed embedding for
                 each word in the vocabulary


      Dynamic embedding: the vector for each
      word is different in different contexts


Satoshi Nakamura, CUHKSZ CSC3160       10/22/25      55
                                     Word2vec
     Popular embedding method
     Very fast to train
     Idea: predict rather than count
     Word2vec provides various options. We'll do:
                  skip-gram with negative sampling (SGNS)




Satoshi Nakamura, CUHKSZ CSC3160                            10/22/25   56
                      Word2vec (Mikolov et al., 2013)
                                                      ‚Ä¢ CBOW: use the surrounding
                                                        words to predict a center
                                                        word. Continuous ‚ÄúBag of
                                                        ‚ÄúWords


                                                      ‚Ä¢ Ski-gram: use the middle
                                                        word to predict surrounding
                                                        words in a window

                                                                                57
   Satoshi
Satoshi    Nakamura@SDS
        Nakamura,           AIR6001/MDS6105/CSC5010
                  CUHKSZ CSC3160                                 10/22/25       57
                      Skip-gram (Mikolov et al., 2013)

                                                      ùë¶! = ùë†ùëúùëìùë°ùëöùëéùë• ùëä‚Ä≤ ‚àó ‚Ñé




                                                      ùë¶! = ùë†ùëúùëìùë°ùëöùëéùë• ùëä‚Ä≤ ‚àó ‚Ñé




                                                      ùë¶! = ùë†ùëúùëìùë°ùëöùëéùë• ùëä‚Ä≤ ‚àó ‚Ñé
                ùëä!‚àó# : map ùë•$ ùë°ùëú ‚Ñé$
                ùëä‚Ä≤#‚àó! : map ‚Ñé$ ùë°ùëú ùë¶%
                       Estimate ‚à∂ ùëä!‚àó# , ùëä‚Ä≤#‚àó!
                                                                            58
   Satoshi
Satoshi    Nakamura@SDS
        Nakamura,           AIR6001/MDS6105/CSC5010
                  CUHKSZ CSC3160                      10/22/25              58
                     Skip-gram with negative samples




Satoshi Nakamura, CUHKSZ CSC3160              10/22/25   59
                                        Word2vec
     Instead of counting how often each word w occurs near "apricot"
        ‚Ä¢ Train a classifier on a binary prediction task:
          ‚Ä¢ Is w likely to show up near "apricot"?

     We don‚Äôt actually care about this task
          ‚Ä¢ But we'll take the learned classifier weights as the word embeddings

     Big idea: self-supervision:
          ‚Ä¢ A word c that occurs near apricot in the corpus acts as the gold "correct
            answer" for supervised learning
          ‚Ä¢ No need for human labels
          ‚Ä¢ Bengio et al. (2003); Collobert et al. (2011)

Satoshi Nakamura, CUHKSZ CSC3160                                 10/22/25           60
         Approach: predict if candidate word c is a "neighbor"

     1. Treat the target word t and a neighboring context
        word c as positive examples.
     2. Randomly sample other words in the lexicon to get
        negative examples
     3. Use logistic regression to train a classifier to distinguish
        those two cases
     4. Use the learned weights as the embeddings

Satoshi Nakamura, CUHKSZ CSC3160                    10/22/25       61
                                     Skip-Gram Training Data

                Assuming a +/- 2 word window, given training sentences


                                   ‚Ä¶lemon, a [tablespoon of apricot jam, a] pinch‚Ä¶
                                         c1          c2      c3       c4
                                                           [target]




Satoshi Nakamura, CUHKSZ CSC3160                                           10/22/25   62
                                        Skip-Gram Classifier
     (assuming a +/- 2 word window)

      ‚Ä¶lemon, a [tablespoon of apricot jam, a] pinch‚Ä¶
                                   c1        c2     c3       c4
                                                  [Traget]
     Goal: train a classifier that is given a candidate (word, context) pair
                      (apricot, jam)
                      (apricot, aardvark)
                     ‚Ä¶
     And assigns each pair a probability:
                   P(+|w, c)
                   P(‚àí|w, c) = 1 ‚àí P(+|w, c)
Satoshi Nakamura, CUHKSZ CSC3160                                  10/22/25     63
            Skip-gram and Skip-gram with Negative Sampling




Satoshi Nakamura, CUHKSZ CSC3160               10/22/25      64
                                           Semantic similarity




                          Example: Vectors in Embedding Representation Space by Word2vec
Satoshi Nakamura, CUHKSZ CSC3160                                                     10/22/25   65
                                        Summary
     ‚Ä¢ Word sense and their relations


     ‚Ä¢ Word representation
          ‚Ä¢ We focus on sparse representation in today‚Äôs lecture
          ‚Ä¢ Term-document matrix
          ‚Ä¢ Term-term matrix
          ‚Ä¢ TF-IDF


     ‚Ä¢ Measure semantic similarity
     ‚Ä¢ Word2Vec
          ‚Ä¢ Skipgram


Satoshi Nakamura, CUHKSZ CSC3160                                   10/22/25   66
                                      Reading and tools

     ‚Ä¢ Word embedding colab
          ‚Ä¢ https://colab.research.google.com/github/pytorch/tutorials/blob/gh-
            pages/_downloads/363afc3b7c522e4e56981679c22f1ad6/word_embeddings_tutorial.ipynb
          ‚Ä¢ https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_e
            mbeddings.ipynb



     ‚Ä¢ Chapter 6: Vector Semantics and Embeddings
          ‚Ä¢ https://web.stanford.edu/~jurafsky/slp3/5.pdf




Satoshi Nakamura, CUHKSZ CSC3160                                           10/22/25              67
             Appendex: CFG vs CSG:
             Context-Sensitive Grammar




Satoshi Nakamura, CUHKSZ CSC3160         10/22/25   68
       Context-Free Grammar vs Context-
       Sensitive Grammar

      With concrete examples (a^n b^n) and (a^n b^n c^n)
      Formal Languages & Automata




Satoshi Nakamura, CUHKSZ CSC3160             10/22/25      69
                                   Big Picture
      ‚Ä¢ CFG (Context-Free): rules of the form A ‚Üí Œ≥ (single
        nonterminal on the left)
      ‚Ä¢ CSG (Context-Sensitive): rules of the form Œ±AŒ≤ ‚Üí Œ±Œ≥Œ≤ (context
        may appear; length non-decreasing)
      ‚Ä¢ CSG ‚äÉ CFG: CSG is strictly more expressive than CFG




Satoshi Nakamura, CUHKSZ CSC3160                      10/22/25      70
        CFG Example: L = { a^n b^n | n ‚â• 0 }
      ‚Ä¢ Equal number of a's and b's
      ‚Ä¢ Classic context-free language; simple grammar suffices


      Grammar:
      S ‚Üí a S b | Œµ

      Derivation (n=3):
      S ‚áí a S b ‚áí a a S b b ‚áí a a a S b b b ‚áí a a a b b b
Satoshi Nakamura, CUHKSZ CSC3160                    10/22/25     71
                                   Why L is Context-Free

      ‚Ä¢ Each expansion adds one a on the left and one b on the right
      ‚Ä¢ Invariant: |a| = |b| is maintained by recursive structure
      ‚Ä¢ Left-hand side is always a single nonterminal (S)




Satoshi Nakamura, CUHKSZ CSC3160                       10/22/25     72
      CSG Example: L = { a^n b^n c^n | n
                    ‚â•1}
    ‚Ä¢ Requires synchronizing counts across three symbols
    ‚Ä¢ Not context-free (pumping lemma for CFLs)
    ‚Ä¢ But context-sensitive (accepted by linear bounded automata)




Satoshi Nakamura, CUHKSZ CSC3160                    10/22/25        73
                              One CSG for a^n b^n c^n
                                   (length non-decreasing)
      ‚Ä¢ Use temporary markers B and C and context to synchronize
        counts
      ‚Ä¢ All rules are non-contracting (|rhs| ‚â• |lhs|)

       1) S ‚Üí a S B C | a b c
       2) C B ‚Üí B C
       3) a B ‚Üí a b
       4) b B ‚Üí b b
       5) b C ‚Üí b c
       6) c C ‚Üí c c

Satoshi Nakamura, CUHKSZ CSC3160                         10/22/25   74
                                   Derivation Sketch for n = 3
     ‚Ä¢ Build stacks of a¬∑B¬∑C, finalize the last triple as abc
     ‚Ä¢ Shuffle markers (CB ‚Üí BC), then finalize locally with context
       rules
                    S
                    ‚áí a S B C
                    ‚áí a a S B C B C
                    ‚áí a a a b c B C B C
                    ‚áí a a a B B B C C C
                    ‚áí a a a b b b C C C
                    ‚áí a a a b b b c c c
Satoshi Nakamura, CUHKSZ CSC3160                            10/22/25   75
                         CFG vs CSG: What Differs

      ‚Ä¢ Rule shape: CFG forbids context on the left; CSG allows it
      ‚Ä¢ Power: CFG handles two-way dependencies; CSG handles
        three-way and copying patterns
      ‚Ä¢ Parsing: CFG parsing is polynomial (e.g., CYK O(n^3)); CSG
        recognition is far more expensive



Satoshi Nakamura, CUHKSZ CSC3160                      10/22/25       76
                                   Quick Checklist

      ‚Ä¢ Two synchronized blocks ‚Üí likely CFG
      ‚Ä¢ Three or more synchronized blocks ‚Üí likely needs CSG
      ‚Ä¢ Copying/duplication ‚Üí beyond CFG




Satoshi Nakamura, CUHKSZ CSC3160                     10/22/25   77
                                   Summary

      ‚Ä¢ CFG: single-nonterminal LHS; efficient parsing
      ‚Ä¢ CSG: contextual constraints; strictly more expressive
      ‚Ä¢ a^n b^n is CFG; a^n b^n c^n is CSG (not CFG)




Satoshi Nakamura, CUHKSZ CSC3160                         10/22/25   78
