   â€£ Attendance registration App:



     Satoshi  CSC3160 Wed. 8:30-9:50                  TB102
     Nakamura         Fri. 10:30-11:50




 â€¢ It's active 15 minutes before class starts until 20 minutes
   after class start time.

 â€¢ Itâ€™s active on two time slots on Wednesdays
   and Fridays; any mismatched check-ins will be
   filtered out.

Satoshi Nakamura, CUHKSZ CSC3160                                 10/22/25   1   1
CSC3160 - Fundamentals of Speech and Language
          Processing



      Lecture 12: Language models


                    Satoshi Nakamura



                                       10/22/25   2
                                            Schedule
                  2025       Date    Class No. Topics
                     9        3          1     Lecture 1 Introduction
                              5          2     Lecture 2 Sound and acoustics
                                               Lecture 3 Lecture 3 Understanding human speech
                               10        3
                                               production
                               12        4     Lecture 4 Speech Perception
                               17        5     Lecture 5 Digital Speech Processing
                               19        6     Lecture 6 Speech representation
                               24        7     Lecture 7 Phonetics
                               26        8     Lecture 8 Text processing (No speech coding )
         (make-up)             28        9     Lecture 9 Words
                 10            (1)   Holiday
                               (3)   Holiday
                               (8)   Holiday
                               10       10     Lecture 10 Syntax
       (make-up?)              11       11     No class
                               15       12     Lecture 11 Word embeddings
                               17       13     Lecture 12 Language models
                               22       14     Midterm Summary

Satoshi Nakamura, CUHKSZ CSC3160                                            10/22/25            3   3
                                            Schedule
                                                                              Attention:
                                                                          Schedule changed
                 2024        Date   Class No. Topics
                               24      15    Midterm Exam
                               29      16    Lecture 13 Statistical MT
                               31      17    Lecture 14 Neural MT (Encoder-Decoder)
              11                5      18    Lecture 15 Transformer
                                7      19    Lecture 16 Self Supervised Modeling
                               12      20    No class (Absent for International Conference)
                               14      21    Lecture 17 Automatic Speech Recognition
                               19      22    Lecture 18 Text to Speech Synthesis
                               21      23    Lecture 19 Chatbot
                               26      24    Lecture 20 QA
                               28      25    Lecture 21 LLM
              12                3      26    Lecture 22 Affects
                                5      27    Summary for the Final Exam
                                             Report Review (SN: Absent for International
                               10      28
                                             Conference)
                               12      29    Final Exam
Satoshi Nakamura, CUHKSZ CSC3160                                         10/22/25             4   4
                                   Assignment Plan


            Assignment   Scope    Release Deadlin Answer session+
                 No                          e       Tutorial
                  1     Class 1-7   9/26   10/10      10/17
                  2    Class 8-12   9/28   10/17      10/22
                  3    Class 16-22 11/1    11/26      11/28
                  4      Report    10/24    12/3      12/10




Satoshi Nakamura, CUHKSZ CSC3160                     10/22/25       5   5
                           Tutrial
â€¢ Oct. 17 18:00-18:50 in the Teaching A TA310 (Friday)
â€¢ Oct. 22 19:00-20:00 in the Teaching B TB102 (Wednesday)




                                                            6
                                               10/22/25         6
                                   Midterm Exam
   â€¢ Mid-term exam: October 24 10:30-12:30
                              th

         â€¢ Scope: Lecture 1- 12 (all lectures until the midterm exam)

         â€¢ Condition
              â€¢ No mobile phone, Pad, or PC.
              â€¢ 1-page both-side cheat sheet
              â€¢ no magnifying glass




Satoshi Nakamura, CUHKSZ CSC3160                               10/22/25   7   7
                           Agenda
â€¢ Recap
â€¢ Word sense and their relations
â€¢ Word representation and embedding
â€¢ Measuring semantic similarity




                                      10/22/25   8
                              Outline
â€¢ Recap
â€¢ What is a language model?
â€¢ Applications of language models
â€¢ N-gram and chain rule
  â€¢ Examples for bigram probabilities
â€¢ Evaluating language models
â€¢ Smoothing



                                        10/22/25   9
           Distributional Hypothesis (Lenci, 2008)


  â€¢ At least certain aspects of the meaning of lexical expressions
   depend on their distributional properties in the linguistic contexts


  â€¢ The degree of semantic similarity between two linguistic
   expressions is a function of the similarity of the two linguistic
   contexts in which they can appear


Satoshi Nakamura@SDS AIR6001/MDS6105/CSC5010                              10
                                   Word representation
     â€¢ Five-word vocabulary: man, walk, woman, swim, ask
          â€¢ 1-of-N encoding/one-hot encoding

               â€¢ [1, 0, 0, 0, 0]: man
               â€¢ [0, 1, 0, 0, 0]: walk
               â€¢ [0, 0, 1, 0, 0]: woman
               â€¢ [0, 0, 0, 1, 0]: swim
               â€¢ [0, 0, 0, 0, 1]: ask


Satoshi Nakamura, CUHKSZ CSC3160                         10/22/25   11
                   Words as vectors: Word dimensions
     â€¢ word-word co-occurrence matrix




Satoshi Nakamura, CUHKSZ CSC3160             10/22/25   12
                                   Spatial visualization




Satoshi Nakamura, CUHKSZ CSC3160                           10/22/25   13
                      Word2vec (Mikolov et al., 2013)
                                                      â€¢ CBOW: use the surrounding
                                                        words to predict a center
                                                        word. Continuous â€œBag of
                                                        â€œWords


                                                      â€¢ Ski-gram: use the middle
                                                        word to predict surrounding
                                                        words in a window

                                                                                14
   Satoshi
Satoshi    Nakamura@SDS
        Nakamura,           AIR6001/MDS6105/CSC5010
                  CUHKSZ CSC3160                                 10/22/25       14
                                           Semantic similarity




                          Example: Vectors in Embedding Representation Space by Word2vec
Satoshi Nakamura, CUHKSZ CSC3160                                                     10/22/25   15
                              Outline
â€¢ Recap
â€¢ What is a language model?
â€¢ Applications of language models
â€¢ N-gram and chain rule
  â€¢ Examples for bigram probabilities
â€¢ Evaluating language models
â€¢ Smoothing



                                        10/22/25   16
        Give a word




The student is watching




                          10/22/25   17
           Probabilistic language model

â€¢ Goal: Compute the probability of a sentence or sequence of words

                  ğ‘ƒ(ğ‘Š) = ğ‘ƒ(ğ‘¤! , ğ‘¤" , ğ‘¤# , . . . , ğ‘¤$ )
â€¢ Probability of an upcoming word

                   ğ‘ƒ(ğ‘¤$ |ğ‘¤! , ğ‘¤" , ğ‘¤# , . . . , ğ‘¤$%! )



                                                         10/22/25   18
                            LM applications
â€¢ Machine translation

             ğ‘ƒ(Students from my class are the best|æˆ‘ç­ä¸Šçš„å­¦ç”Ÿæ˜¯æœ€æ£’çš„)
             > ğ‘ƒ(Students from Stanford are the best|æˆ‘ç­ä¸Šçš„å­¦ç”Ÿæ˜¯æœ€æ£’çš„)

â€¢ Natural language generation
    ğ‘ƒ(best|Students from my class are the) > ğ‘ƒ(average|Students from my class are the)

â€¢ Speech recognition

                      ğ‘ƒ(Three students) > ğ‘ƒ(Tree students)



                                                                   10/22/25              19
Language models in daily life




                         10/22/25   20
                   Probability of next word


                                            ğ¶(Students from my class are the best)
   ğ‘ƒ(best|Students from my class are the) =
                                              ğ¶(Students from my class are the)




â€¢ C(Students from my class are the best) is the count of the phrase
  â€œStudents from my class are the bestâ€



                                                               10/22/25              21
                  Probability of next word
 â€¢ Chain rule of probability
              ğ‘ƒ(ğ‘¤!:# ) = ğ‘ƒ(ğ‘¤! )ğ‘ƒ(ğ‘¤$ |ğ‘¤! )ğ‘ƒ(ğ‘¤% |ğ‘¤!:$ ). . . ğ‘ƒ(ğ‘¤# |ğ‘¤!:#&! )

â€¢ Smarter way to estimate the probability
    â€¢ Use conditional independence

   ğ‘ƒ(Students from my class are the best)
   = ğ‘ƒ(best|the)ğ‘ƒ(the|are)ğ‘ƒ(are|class)ğ‘ƒ(class|my)ğ‘ƒ(my|from)ğ‘ƒ(from|Students)ğ‘ƒ(Students)




                                                             10/22/25             22
           N-gram


The student is watching
Unigram: â€œ-â€
Bigram: â€œwatching - â€
Trigram: â€œis watching -â€
4-gram: â€œstudent is watching -â€
5-gram: â€œThe student is watching -â€

                          10/22/25    23
                       Bigram model
â€¢ approximates the probability of a word given all the previous
  words by using only the conditional probability of the preceding
  word


        ğ‘ƒ(best|Students from my class are the) â‰ˆ ğ‘ƒ(best|the)




                                                    10/22/25         24
              Markov assumption

â€¢ Assumption: the probability of a word
  depends only on the previous words
  without looking too far into the past

 ğ‘ƒ(best|Students from my class are the) â‰ˆ ğ‘ƒ(best|the)


         ğ‘ƒ(ğ‘¤# |ğ‘¤!:#&! ) â‰ˆ ğ‘ƒ(ğ‘¤# |ğ‘¤#&! )            1-st order Markov


â€¢ Markov model: assume we can predict the probability of
  some future unit without looking too far into the past
                                                             10/22/25   25
          Generalizing bigram to N-gram
â€¢ From bigram to N-gram


                   ğ‘ƒ(ğ‘¤# |ğ‘¤!:#&! ) â‰ˆ ğ‘ƒ(ğ‘¤# |ğ‘¤#&'(!:#&! )

â€¢ N = 2: bigram
â€¢ N = 3: trigram
â€¢ N = 4: 4-gram
â€¢ N = 5: 5-gram


                                                         10/22/25   26
Simplest case: unigram

ğ‘ƒ(ğ‘¤!:# ) = ğ‘ƒ(ğ‘¤! )ğ‘ƒ(ğ‘¤$ )ğ‘ƒ(ğ‘¤% ). . . ğ‘ƒ(ğ‘¤# )




                                    10/22/25   27
                      Bigram model
â€¢ Condition on the previous word

                ğ‘ƒ(ğ‘¤& |ğ‘¤!:&%! ) â‰ˆ ğ‘ƒ(ğ‘¤& |ğ‘¤&%! )




                                                10/22/25   28
           Example with a mini-corpus



          <s> : beginning symbol
          </s>: ending symbol

â€¢ Maximum-likelihood estimation (MLE): bigram probability




                                                10/22/25    29
                  A slightly large example
 â€¢ Unigram counts




â€¢ Bigram counts
  â€¢ â€œI wantâ€ occurred 827 times in the document.
  â€¢ â€œwant wantâ€ occurred 0 times.

                                                   10/22/25   30
                   Bigram probabilities




â€¢ Other useful probabilities

â€¢ Calculate the probability of sentences like â€œI want English foodâ€




                                                   10/22/25           31
Evaluating language models




Training set       Test set




                         10/22/25   32
                          Perplexity
â€¢ the inverse probability of the test set, normalized by the number
  of words




â€¢ Applying chain rule




                                                   10/22/25           33
                      Intuition of perplexity
â€¢ Intuitively, perplexity can be understood as a measure of uncertainty
â€¢ Whatâ€™s the level of uncertainty in predicting the next word?
   â€¢ The current president of CUHK Shenzhen is _______ ?
   â€¢ ChatGPT is built on top of OpenAI's GPT-5 family of large language _____ ?


â€¢ Uncertainty level
   â€¢ Unigram: highest
   â€¢ Bigram: high
   â€¢ 5-gram: low




                                                           10/22/25               34
Lower perplexity = better model




https://web.stanford.edu/~jurafsky/slp3/3.pdf

https://www.isca-speech.org/archive_v0/Interspeech_2017/pdfs/0729.PDF
                                                                        10/22/25   35
Long tail




            10/22/25   36
                The perils of overfitting
â€¢ N-gram models only work well for word prediction if the test
  corpus looks like the training corpus
  â€¢ In the real world, the inference corpus often doesnâ€™t look like the
    training
  â€¢ Robust models that generalize are all we need
  â€¢ One kind of generalization: Zeros
     â€¢ Things that donâ€™t ever occur in the training set but not in the
       test set


                                                    10/22/25             37
                               Zeros
â€¢ Training set
                                   â€£ Test set
  â€¢ â€¦ denied the allegations
                                     - â€¦ denied the offer
  â€¢ â€¦ denied the reports
                                     - â€¦ denied the loan
  â€¢ â€¦ denied the claims
  â€¢ â€¦ denied the request

                      ğ‘ƒ(offer|denied the) = 0

                      ğ‘ƒ(loan|denied the) = 0


                                                  10/22/25   38
                  Zero probability bigrams
â€¢ Bigram with zero probability
  â€¢ On test set
                   ğ‘ƒ(ğ‘¤& |ğ‘¤!:&%! ) â‰ˆ ğ‘ƒ(ğ‘¤& |ğ‘¤&%! )

â€¢ Perplexity: canâ€™t compute because of 1 over 0â€¦




                                                   10/22/25   39
            Unseen events
Training data: The wolf is an endangered species
Test data:      The wallaby is endangered




                                            https://courses.engr.illinois.edu/cs447
                                            /fa2018/Slides/Lecture04.pdf

                                            10/22/25                           40
What can we do?




                  10/22/25   41
Dealing with unknown words: Simple solution
â€¢ Create an unknown word token <UNK>
  â€¢ Training of <UNK> probabilities
  â€¢ Create a fixed lexicon L of size V
  â€¢ At text normalization phase, any training word not in L
    changed to <UNK>


â€¢ During inference
  â€¢ Use UNK probabilities for any word not in training


                                                   10/22/25   42
                                Smoothing
â€¢ To improve the accuracy of our model
â€¢ To handle data sparsity, out of vocabulary words, words that are absent in the
  training set.
â€¢ Smoothing techniques
   â€¢ Laplace smoothing: Also known as add-1 smoothing
   â€¢ Additive smoothing
   â€¢ Good-turing smoothing
   â€¢ Kneser-Ney smoothing
   â€¢ Katz smoothing
   â€¢ Church and Gale Smoothing

â€¢ Interpolation of the models

                                                             10/22/25              43
                 Laplace Smoothing
â€¢ Assuming every (seen or unseen) event occurred once more
  than it did in the training data.



                                      ğ¶(ğ‘¤678 , ğ‘¤6 ) + 1
               ğ‘ƒLaplace (ğ‘¤6 |ğ‘¤678 ) =
                                       ğ¶(ğ‘¤678 ) + ğ‘‰




                                              10/22/25       44
           Bigram counts



Original




Smoothed


                           10/22/25   45
                          Intuition of smoothing
â€¢ When we have sparse statistics:
  â€¢ P(w | denied the)
    â€¢ 3 allegations
    â€¢ 2 reports
    â€¢ 1 claims
    â€¢ 1 request
â€¢ Steal probability mass to generalize better
  â€¢ P(w | denied the)
    â€¢ 2.5 allegations
    â€¢ 1.5 reports
    â€¢ 0.5 claims
    â€¢ 0.5 request
    â€¢ 2 other
                                                   10/22/25   46
               Backoff and interpolation
â€¢ Use less context

  â€¢ Backoff
     â€¢ use trigram if you have good evidence,
     â€¢ otherwise bigram, otherwise unigram


  â€¢ Interpolation
     â€¢ Mix unigram, bigram, trigram



                                                10/22/25   47
Self-supervised Traning: Masked Language Model
â€¢BERT(Masked language models)
 â€¢ Consider predicting the next word for the following example:




  â€¢ What if you have more (bidirectional) context?



 â€¢ Information from the future can be helpful for language understanding!

                                                                  22/10/2025   48
  Bidirectional Encoder Representations from Transformers (BERT)




                                    Encoder

BERT = Encoder of Transformer




                                                 10/22/25          49
Masked language model




                   10/22/25   50
Next sentence prediction




                      10/22/25   51
       How to use BERT â€“ Case 1

   class
                                   Input: single sentence,
  Linear           Trained from    output: class
 Classifier        Scratch


hCLS                                   Use cases:
                                       Sentiment analysis
       BERT            Fine-tune       Document Classification


  [CLS]       w1    w2        w3
                   sentence
                                                  10/22/25       52
    How to use BERT â€“ Case 2

        class     class    class

        Linear Linear      Linear   Input: single sentence,
        Cls    Cls         Cls      output: class of each word



                                    Use case: Slot filling
             BERT


[CLS]   w1       w2        w3
                sentence
                                                      10/22/25   53
                                Summary
â€¢ Language model
   â€¢ Compute the probability of a sentence or sequence of words
   â€¢ Predicting next word
â€¢ N-gram
   â€¢ Unigram
   â€¢ Bigram
   â€¢ Trigram
   â€¢ Etc
â€¢ Evaluating language model: perplexity
â€¢ Smoothing
â€¢ BERT


                                                             10/22/25   54
                                       Reading
â€¢ Chapter 3: N-gram Language Models
   â€¢ https://web.stanford.edu/~jurafsky/slp3/3.pdf




                                                     10/22/25   55
