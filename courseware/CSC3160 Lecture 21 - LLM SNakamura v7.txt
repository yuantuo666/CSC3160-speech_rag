                         CTE Questionnaire



       ‚Ä£ Attendance registration App:
         Satoshi  CSC3160 Wed. 8:30-9:50                              TB102
         Nakamura         Fri. 10:30-11:50




‚Ä¢ It's active 15 minutes before class starts until 20
  minutes after class start time.
‚Ä¢ It‚Äôs active on two time slots on Wednesdays and
  Fridays; any mismatched check-ins will be filtered out.


Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201           2025/12/1
                                                                                          1
Lecture 21: Large
Language Models
Satoshi Nakamura




                   CTE Questionnaire
                       Schedule
 2024   Date   Class No. Topics
         24       15    Midterm Exam
         29       16    Lecture 13 Statistical MT
         31       17    Lecture 14 Neural MT (Encoder-Decoder)
11        5       18    Lecture 15 Transformer
          7       19    Lecture 16 Self Supervised Modeling
         12       20    No class (Absent for International Conference)
         14       21    Lecture 17 Automatic Speech Recognition
         19       22    Lecture 18 Text to Speech Synthesis
         21       23    Lecture 19 Chatbot and Dialogue System
         26       24    Lecture 20 QA
         28       25    Lecture 21 LLM
12        3       26    Lecture 22 Affects
          5       27    Summary for the Final Exam
                        Report Review (SN: Absent for International
         10       28
                        Conference)
         12       29    Final Exam                CTE Questionnaire
                                                                  2025/12/1
                                                                              3
                Assignment Plan


Assignment     Scope     Release Deadline Answer session+
     No                                      Tutorial
      1       Class 1-7    9/26   10/10       10/17
      2      Class 8-12    9/28   10/21      (10/22)
      3      Class 13-21   11/7    12/3       11/28
      4        Report     10/24    12/3       12/10



                                        CTE Questionnaire
                                                      2025/12/1
                                                                  4
                      Condition of Final Exam
‚Ä¢ Time: 10:30-12:30, December 12, 2025
‚Ä¢ Venue: Liwen Hall
‚Ä¢ Shut off your mobile phones and all other electronic devices (smart watches, ..)
  and bring them in front of the room.
‚Ä¢ Place your student ID card on your desk.
‚Ä¢ You have 120 minutes to complete the exam.
‚Ä¢ You are allowed to bring one self-made sheet of A4 paper (with arbitrary notes on
  both sides) for your personal use in this exam. You can use software or suitable
  devices to generate/create your cheat sheet (including tablets, iPads, etc) and
  bring a printed version to the exam.
‚Ä¢ Other tools and electronic devices are not allowed. Failure to comply with these
  rules will be recorded.

                                                           CTE Questionnaire
                                                                            2025/12/1
                                                                                        5
                             Continued
‚Ä¢ If you need extra sheets, call us. Please remember to write all answers in
  the answer book. We will only check your answer book when scoring.
‚Ä¢ Please do not use red or green pens or a pencil.
‚Ä¢ If you need to go to the toilet, call us. Only one person may leave the
  room at a time.
‚Ä¢ An announcement will be made 10 minutes before the official
  examination time ends. From that point on, no one will be allowed to
  leave the room to minimize distraction.
‚Ä¢ If you finish the exam before the last 10 minutes, you may leave the class
  room after handing us your complete exam sheet, answer book and
  notes.
                                                       CTE Questionnaire
                                                                      2025/12/1
                                                                                  6
                            Makeup Exam
‚Ä¢ If you have unavoidable conflicts or illness, you can request a
  makeup exam. Keep in mind the following if you want to request
  the make-up exam.
  ‚Ä¢ The total score is adjusted to 80.
  ‚Ä¢ The request should be sent by 23:59, Dec. 11.
  ‚Ä¢ The makeup exam must take place by Dec. 15.
  ‚Ä¢ When you request this, you must send us emails with evidence of your
    situation and the preferred time for the exam.
‚Ä¢ Argument Session
  ‚Ä¢ We will provide an opportunity for an argument for the
    Final Exam later.
                                                       CTE Questionnaire
                                                                       2025/12/1
                                                                                   7
                                   Contents
   ‚Ä¢ Recap
   ‚Ä¢ What are the Large Language Models
   ‚Ä¢ How large are they.
   ‚Ä¢ Why LLMs
   ‚Ä¢ Emergent capability
   ‚Ä¢ In-context-learning
   ‚Ä¢ Chain of thoughts
   ‚Ä¢ Zero-shot/few-shot COT training

                                              CTE Questionnaire
Satoshi Nakamura, CSC3160     01/12/2025                          8
           What is question answering?
‚Ä¢ The goal of question answering is to build systems that
  automatically answer questions posed by humans in a natural
  language



            Question        QA system        Answer




                                            CTE Questionnaire
                                                                9
                         Reading comprehension: BERT




                                 Credit: https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/   CTE Questionnaire
CSC 3160, Satoshi Nakamura, CUHKSZ                                                                                                     2025/12/1
                                                                                                                                                   10
Reading comprehension: BERT




  Credit: https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/   CTE Questionnaire
                                                                                                              11
                          Entity linking
‚Ä¢ Done in (roughly) two stages: mention detection and mention
  disambiguation




                                                  CTE Questionnaire
                                                                      12
                          Large Language Models for QA
                                         M: Mask




                                                   CTE Questionnaire
CSC 3160, Satoshi Nakamura, CUHKSZ                               2025/12/1
                                                                             13
                                   Contents
   ‚Ä¢ Recap
   ‚Ä¢ What are the Large Language Models
   ‚Ä¢ How large are they.
   ‚Ä¢ Why LLMs
   ‚Ä¢ Emergent capability
   ‚Ä¢ In-context-learning
   ‚Ä¢ Chain of thoughts
   ‚Ä¢ Zero-shot/few-shot COT training

                                              CTE Questionnaire
Satoshi Nakamura, CSC3160     01/12/2025                          14
          Slide credit:
Slide credit: Prof. Benyou Wang
       CSC6203/CIE6201


                       CTE Questionnaire
                                           15
What are Large Language models (LLMs)?




                           CTE Questionnaire
                                               16
                                                      Background


                ‚Ä¢ language model




                                Liu et al., Representation Learning for Natural Language Processing, Springer, 2020


Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                   2025/12/1
                                                                                                                                  17
                               What is language modeling?
           A language model assigns a probability to a N-gram
                                        !    "
                                    ùëì: ùëâ ‚Üí ùëÖ




                                                                      CTE Questionnaire
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                 2025/12/1
                                                                                                18
                            What is language modeling?
           A language model assigns a probability to a N-gram
                                        !    "
                                    ùëì: ùëâ ‚Üí ùëÖ




                                                       Sfklkljf fskjhfkjsh kjfs fs kjhkjhs fsjhfkshkjfh          Low probability




                                                      ChatGPT is all you need                                    high probability




                                                                                                          CTE Questionnaire
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                          2025/12/1
                                                                                                                                         19
                            What is language modeling?
           A language model assigns a probability to a N-gram
                                        !    "
                                    ùëì: ùëâ ‚Üí ùëÖ


         A conditional language model assigns a probability of a word given
         some conditioning context
                                                                      !#$              "
                                                              ùëî: (ùëâ         , ùëâ) ‚Üí ùëÖ
                                                                                            '()! ‚ãØ)" )
         And             ùíë ùíòùíè ùíòùüè ‚ãØ ùíòùíè#ùüè ) = ùëî(ùë§$ ‚ãØ ùë§!#$ , ùë§) =
                                                                                           '()! ‚ãØ)"#! )




                                                                                             CTE Questionnaire
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                        2025/12/1
                                                                                                                       20
                            What is language modeling?
           A language model assigns a probability to a N-gram
                                        !    "
                                    ùëì: ùëâ ‚Üí ùëÖ


         A conditional language model assigns a probability of a word given
         some conditioning context
                                                                      !#$              "
                                                              ùëî: (ùëâ         , ùëâ) ‚Üí ùëÖ
                                                                                            '()! ‚ãØ)" )
         And             ùíë ùíòùíè ùíòùüè ‚ãØ ùíòùíè#ùüè ) = ùëî(ùë§$ ‚ãØ ùë§!#$ , ùë§) =
                                                                                           '()! ‚ãØ)"#! )




                      ùíë ùíòùíè ùíòùüè ‚ãØ ùíòùíè#ùüè ) is the foundation of modern large language models (GPT, ChatGPT, etc.)

                                                                                             CTE Questionnaire
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                             2025/12/1
                                                                                                                            21
            Language model using neural networks
                                                                                        outputÔºö   Âú®


         GPT-3/ChatGPT/GPT4 have
                                                      Back-box neural networksÔºö
         175B+ parameters
         Humans have 100B+ neurons




                                                     inputÔºö      Êàë        ÊÄù       ÊïÖ         Êàë

                                                                                      CTE Questionnaire
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                   2025/12/1
                                                                                                                  22
                                        Language models: Narrow Sense
         A probabilistic model that assigns a probability to every Ô¨Ånite sequence (grammatical or not)




         GPT-3 still acts in this way but the model is implemented as a very large neural network of 175-
         billion parameters!


Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                            2025/12/1
                                                                                                           23
                         Language models: Broad Sense
                        Decoder-only models (GPT-x models)
                                                                        The latter two usually
                 ‚ùñ


                 ‚ùñ      Encoder-only models (BERT, RoBERTa, ELECTRA)
                 ‚ùñ      Encoder-decoder models (T5, BART)
                                                                       involve a different pre-
                                                                          training objective.




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                 2025/12/1
                                                                                                24
                                                             PLM vs. LLM
                                                                          We do not explicitly mention pre-training because
                                                                          pre-training and training use the same language
                                                                          models objective (e.g., autoregressive generation)

              ‚Ä¢ Pre-trained language model
              ‚Ä¢ Large pre-trained Language Model (LLM)




                       Image source: https://liuquncn.github.io/talks/20220228-Thai-AI-engineer-group/Huge-Pre-trained-Language-Models.public.pdf


Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                                 2025/12/1
                                                                                                                                                                25
                                     How Large are ‚ÄúLarge‚Äù LMs?




     More recent models: PaLM (540B), OPT (175B), BLOOM (176B)‚Ä¶
      https://huggingface.co/blog/large-language-models               Image source: https://hellofuture.orange.com/en/the-gpt-3-language-model-revolution-or-evolution/

Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                               2025/12/1
                                                                                                                                                                          26
                           Large Language Models ‚Äì
                            yottaFlops of Compute




https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf 1 yotta =   1024 FLOPs: floating point operations
 Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                         2025/12/1
                                                                                                                                          27
                                     Large Language Models ‚Äì
                                     Hundreds of Billions of Tokens




             https://babylm.github.io/
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  28
           Some basics for large language models
        ‚Ä¢ Scalable network architecture (Transformer vs. CNN/RNN)

        ‚Ä¢ Scalable objective (conditional/auto-regressive LM vs. Masked LM)


                                    Loss




                                                                                             Model size




        ‚Ä¢ Scalable data (plain texts are everywhere vs. supervised data)
              ‚Ä¢ https://github.com/esbatmop/MNBVC

                   OpenAI, GPT-4 Technical Report, https://cdn.openai.com/papers/gpt-4.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                       2025/12/1
                                                                                                                      29
                                        How Large are ‚ÄúLarge‚Äù LMs?

             ‚ùñ      Today, we mostly talk about two models:
                       ‚û¢Medium-sized models: BERT/RoBERTa models (100M or 300M), T5 models
                        (220M, 770M, 3B)
                       ‚û¢‚ÄúVery‚Äù large LMs: models of 100+ billion parameters

             ‚ùñ      Larger model sizes needs larger computation, more expensive during inference
             ‚ùñ      Different sizes of LMs have different ways to adapt and use them
                       ‚û¢ Fine-tuning, zero-shot/few-shot prompting, in-context learning‚Ä¶
             ‚ùñ      Emergent properties arise from model scale
             ‚ùñ      Trade-off between model size and corpus size




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                2025/12/1
                                                                                                               30
                                                                      Why LLMs?




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201               2025/12/1
                                                                                              31
                              Why Larger language models
    ‚Ä¢ More world knowledge (LAMA)
         ‚Ä¢ Language models as knowledge base?

    ‚Ä¢ Larger capacity to learn problem-solving Abilities
         ‚Ä¢ Coding, revising articles, reasoning etc.

    ‚Ä¢ Better generalization to unseen tasks




    ‚Ä¢ Emergent ability

    Jared Kaplan et. al Scaling Laws for Neural Language Models
    Jason Wei et. Al. Emergent Abilities of Large Language Models.

Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  32
                                                                      Why LLMs?
         Generalization :
            One single model to solve many NLP tasks




                                                                                  https://arxiv.org/pdf/1910.10683.pdf




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                      2025/12/1
                                                                                                                         33
                                                                      Why LLMs?
      Emergent properties in LLMs:
         Some ability of LM is not present in smaller models but is present in larger models
      Emergent Capability: Few-shot prompting




          > A few-shot prompted task is emergent if it
          achieves random accuracy for small models
          and above-random accuracy for large
          models.



      https://docs.google.com/presentation/d/1yzbmYB5E7G8lY2-KzhmArmPYwwl7o7CUST1xRZDUu1Y/edit?resourcekey=0-6_TnUMoK WCk_FN2BiPxmbw#slide=id.g1fc34b3ac18_0_27
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                                          2025/12/1
                                                                                                                                                                         34
                                                                      Why LLMs?
          ‚óè      Emergent Abilities
                    ‚óã     Some ability of LM is not present in smaller models but is present in larger models




              https://docs.google.com/presentation/d/1yzbmYB5E7G8lY2-KzhmArmPYwwl7o7CUST1xRZDUu1Y/edit?resourcekey=0-6_TnUMoK
              WCk_FN2BiPxmbw#slide=id.g1fc34b3ac18_0_27
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                        2025/12/1
                                                                                                                                       35
                                                Emergent Capability ‚Äì
                                                  In-Context Learning




                                                           https://arxiv.org/pdf/2005.14165.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                                                  2025/12/1
                                                                                                              36
                                                Emergent Capability ‚Äì
                                                  In-Context Learning




          https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec04.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                                           2025/12/1
                                                                                                       37
                                                Emergent Capability ‚Äì
                                                  In-Context Learning




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                        2025/12/1
                                                                                    38
                                         Emergent Capability ‚Äì
                                       Chain of Thoughts Prompting




          https://arxiv.org/pdf/2201.11903.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                     2025/12/1
                                                                                 39
                                        Emergent Capability ‚Äì
                                      Chain of Thoughts Prompting




              https://arxiv.org/pdf/2201.11903.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  40
                                               Emergent Capability ‚Äì
                                               Zero Shot CoT Prompting




          https://arxiv.org/pdf/2205.11916.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                         2025/12/1
                                                                                     41
                                           Emergent Capability ‚Äì
                                          Self-Consistency Prompting




             https://arxiv.org/pdf/2203.11171.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                       2025/12/1
                                                                                   42
                                                Emergent Capability ‚Äì
                                                Least-to-Most Prompting




     https://arxiv.org/pdf/2205.10625.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                          2025/12/1
                                                                                      43
                    Emergent Capability ‚Äì
                 Augmented Prompting Abilities
        Advanced Prompting Techniques                                     Ask a human to


           ‚óè     Zero-shot CoT Prompting                              ‚óè   Explain the rationale
           ‚óè     Self-Consistency                                     ‚óè   Double check the answer
           ‚óè     Divide-and-Conquer                                   ‚óè   Decompose to easy subproblems



          Large Language Models demonstrate some human-like behaviors!



Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                             2025/12/1
                                                                                                            44
     Emergent Capability - Zero Shot CoT Prompting




          https://arxiv.org/pdf/2205.11916.pdf
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  45
                                                                     Why LLMs?
                                                                                                          Benefit from new technology
     Emergent Capability: Few-shot prompting                                                 RLHF: Reinforcement Learning with Human Feedback




                                                                                                 Bai et al., 2022.
                                                                                                                                                  RLHF helps
                                                                                                                RLHF hurts                        performance
                                                                                                                performance
            https://docs.google.com/presentation/d/1yzbmYB5E7G8lY2-KzhmArmPYwwl7o7CUST1xRZDUu1Y/edit?resourcekey=0-6_TnUMoK WCk_FN2BiPxmbw#slide=id.g1fc34b3ac18_0_27

Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang
CSC6203/CIE6201
                                                                                                                                                              2025/12/1
                                                                                                                                                                          46
                                        To be or not to be Large?
  Inverse scaling can become U-shaped: To be large ?                                                           Inverse Scaling Prize: Not to be large?

          Small language model ‚Üí ‚Äúglib‚Äù                                   Large language model ‚Üí‚Äúglib‚Äù

                                              Medium language model ‚Üí‚Äúgold‚Äù




                                                                                                         See:
                                                                                                           ‚ùñ      TruthfulQA: The largest models were generally
                                                                                                                  the least truthful
                                                                                                           ‚ùñ      https://github.com/inverse-scaling/prize
                                                                                                           ‚ùñ      https://irmckenzie.co.uk/round1


    Inverse scaling can become U-shaped, 2022. J. Wei, Y. Tay, & Q. Le.

Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                                   2025/12/1
                                                                                                                                                                  47
                                                What are ChatGPT and GPT-4?




Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201           2025/12/1
                                                                                          48
                           From 2020 GPT-3 to 2022 ChatGPT




      https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                              2025/12/1
                                                                                                                                                                  49
   RLHF: Reinforcement Learning by Human
                  Feedback
‚Ä¢ fine-tune large language models (LLMs) so that their responses
  better align with human preferences, beyond just learning
  language patterns.
‚Ä¢ It's used in systems like ChatGPT, InstructGPT, and Claude to
  make models more helpful, honest, and harmless.




                                                                   50
                          3-Stage Training Process: RLHF
   ‚Ä¢ 1. Supervised Fine-Tuning (SFT)
         ‚Ä¢ Use instruction‚Äìresponse pairs created by humans. The model learns to
           follow instructions by supervised learning.
         ‚Ä¢ Example:json
         { "instruction": "Translate this sentence into French.", "input": "How are
         you?", "output": "Comment √ßa va ?"}


         ‚Ä¢ This stage is often referred to as instruction tuning.




Satoshi Nakamura@SDS, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201       2025/12/1
                                                                                          51
                          3-Stage Training Process: RLHF
   ‚Ä¢ 2. Reward Model (RM) Training
         ‚Ä¢ Collect multiple model outputs for the same prompt.
         ‚Ä¢ Have humans rank them from best to worst.
         ‚Ä¢ Train a reward model to predict which response a human would prefer.
         ‚Ä¢ Example:
               ‚Ä¢ Prompt: Explain black holes to a 10-year-old.
               ‚Ä¢ Output A: A black hole is...Output B: It's a region of spacetime...


               ‚Ä¢ Human preference: A > B
         ‚Ä¢ ‚Üí Reward model learns to give A a higher score


Satoshi Nakamura@SDS, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                2025/12/1
                                                                                                   52
                          3-Stage Training Process: RLHF
   ‚Ä¢ 3. Reinforcement Learning (PPO)
         ‚Ä¢ Use PPO (Proximal Policy Optimization) to fine-tune the model.
         ‚Ä¢ The model is updated to generate outputs that maximize the reward
           score given by the reward model.
         ‚Ä¢ PPO helps stabilize training and prevent large, harmful policy updates.




Satoshi Nakamura@SDS, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201     2025/12/1
                                                                                        53
Three important abilities that the initial GPT-3 exhibit:
         ‚ùè       Language generation: follow a prompt and then generate a completion of the given prompt.
         ‚ùè       In-context learning: Follow a few examples of a given task and then generate the solution for a new
                 test case.
         ‚ùè       World knowledge: including factual knowledge and commonsense.

 Where do these abilities come from?
          Large-scale pretraining [175B parameters model on 300B tokens]

             ‚óè     Language generation ability comes from the language modeling training objective.
             ‚óè     World knowledge comes from the 300B token training corpora (or where else it could be).
             ‚óè     In-context learning ability, as well as its generalization behavior, is still elusive. There is some studies on why
                   language model pretraining induces in-context learning, and why in-context learning behaves so differently than
                   fine-tuning. Here are some materials, we may spend a lecture focusing on this.
                      a.  https://thegradient.pub/in-context-learning-in-context/ (Highly-recommended)
                      b.  http://ai.stanford.edu/blog/understanding-incontext/
                      c.  https://arxiv.org/abs/2211.15661
                      d.  https://arxiv.org/abs/2212.10559
                      e.    https://arxiv.org/pdf/2209.10063.pdf
  https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                          2025/12/1
                                                                                                                                                              54
                               Code-Davinci-002 & Text-Davinci-002, training on code,
                                              tuning on instructions
                                                                                       New abilities:
                                                                                          ‚ùè Responding to human instruction: previously,
                                                                                            the outputs of GPT-3 were mostly high-frequency
                                                                                            prompt-completion patterns within the training
                                                                                            set. Now the model generates reasonable answers
                                                                                            to the prompt.
                                                                                          ‚ùè Code generation and code understanding:
                                                                                            obviously, because the model is trained on code.
                                                                                          ‚ùè Complex reasoning with chain-of-thought:
                                                                                            previously, the model could not do tasks requiring
                                                                                            multi-step reasoning with chain-of-thought.
                                                                                                  ‚ùè     CoT paper the first version reports that davinci performance
                                                                                                        on GSM8K accuracy 12.4 v.s. the 5th version reports code-
                                                                                                        davinci-002 accuracy 63.1


      Are these abilities already there after pretraining or later injected by fine-tuning?
        ‚ùè     There is still no hard evidence showing training on code is absolutely the reason for
              CoT and complex reasoning.
       https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                                               2025/12/1
                                                                                                                                                                   55
                                                What‚Äôs ChatGPT
    ‚Ä¢ Phase 1: pre-training
         ‚Ä¢ Learn general world knowledge, ability, etc.

    ‚Ä¢ Phase 2: Supervised finetuning
         ‚Ä¢ Tailor to tasks (unlock some abilities)

    ‚Ä¢ Phase 3: RLHF
         ‚Ä¢ Tailor to humans
         ‚Ä¢ Even you could teach ChatGPT to do something


        Most of these were explored by InstructGPT. The only difference is that it is further trained with chat data, as an
        success of product (plus engineering).
     T Schick et. al. Toolformer: language models can teach themselves to use tools. https://arxiv.org/abs/2302.04761
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                                           2025/12/1
                                                                                                                                          56
                                                                            GPT-4
   ‚ùè       Make progress towards multilingualism: GPT-4 is able to answer thousands of multiple-choice questions in 26
           languages with a high degree of accuracy.
   ‚ùè       Longer memory for conversations: ChatGPT can process 4,096 tokens. Once this limit was reached, the model lost
           track. GPT-4 can process 32,768 tokens. Enough for an entire short story on 32 A4 pages.
   ‚ùè       Multimodal input: not only text can be used as input, but also images in which GPT-4 can describe objects. (It is
           not released yet)

GPT-4 Technical Report from OpenAI
       ‚ùè     Only contains a small amount of detail: ‚Äú[...] given both the competitive landscape and the
             safety implications of large-scale models like GPT-4, this report contains no further details about
             the architecture (including model size), hardware, training compute, dataset construction,
             training method or similar.‚Äù From Technical Report.
       ‚ùè     GPT-4‚Äôs score on the bar exam was similar to that of the top ten percent of graduates, while
             ChatGPT ranked in among the ten percent that scored the worst.
       ‚ùè     OpenAI hired more than 50 experts who interacted with and tested the model over an extended
             period of time.

   It was finished in August 2022. It takes 7 months for security alignment
       https://www.adesso.de/en/news/blog/a-brief-introduction-to-gpt-4-2.jsp
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                            2025/12/1
                                                                                                                           57
                                               Open questions
    ‚Ä¢ The source of Reasoning?
         ‚Ä¢ In-context learning
         ‚Ä¢ COT
    ‚Ä¢ Emergent abilityÔºü
    ‚Ä¢ Where is its borderÔºü
    ‚Ä¢ Alignment makes it generalize betterÔºü
    ‚Ä¢ Why so longer context? Can it be longer?
    ‚Ä¢ Continue scaling upÔºü
    ‚Ä¢ Could ‚Äúdata plus RLHF‚Äù achieve AGI? If not, what else?
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  58
                           Difficulties to Replicate ChatGPT
    ‚Ä¢ Computing resources: money is all you need
    ‚Ä¢ Data and annotation:
          ‚Ä¢ Very careful data cleaning„ÄÅ filtering„ÄÅselection strategies (training is expensive)
          ‚Ä¢ Plain corpora(https://github.com/esbatmop/MNBVC)
          ‚Ä¢ Transferable SFT data (instruction tuning)
          ‚Ä¢ human feedback data (model-dependent, non Transferable)
    ‚Ä¢ Algorithms
          ‚Ä¢ Has some open-source implementation in general
          ‚Ä¢ Engineering work is not easy (including training tricks and efficient deployment)
          ‚Ä¢ Releasing a model is easy, keeping polishing it is not!
    ‚Ä¢ Talents (first-tier young researchers, average age of Open AI guys is 32Ôºâ
         <OpenAI ChatGPTÂõ¢ÈòüÂåó‰∫¨Á†îÁ©∂Êä•Âëä>. AminerÂíåÊô∫Ë∞±Á†îÁ©∂.2023.02

Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                    2025/12/1
                                                                                                   59
                                       Well-known strategies
    ‚Ä¢ Probably initialized from a well-trained models
         ‚Ä¢ GLM-130 ÔºàChinese and EnglishÔºâ
         ‚Ä¢ OPTÔºàmainly EnglishÔºâ
         ‚Ä¢ Bloom ÔºàmultilingualÔºâ
         ‚Ä¢ Pangu-alphaÔºàChineseÔºâ
         ‚Ä¢ CPMÔºàChineseÔºâ
         ‚Ä¢ LLaMA (mainly English)
         ‚Ä¢ Alpaca (LLaMA 7b + Self-instruct)
         ‚Ä¢ Chinese- Alpaca
         ‚Ä¢ ChatGLMÔºà6BÔºâ
         ‚Ä¢ Baichuan

    ‚Ä¢ ChatGPT Distillation
         ‚Ä¢ Self-instruct
         ‚Ä¢ Training on ChatGPT conversations

    ‚Ä¢ RL from human feedback


Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201   2025/12/1
                                                                                  60
                               Provide hands-on practice:
  Github Repositories
  ‚Ä¢ nanoGPT https://github.com/karpathy/nanoGPT
  ‚Ä¢ minGPT https://github.com/karpathy/minGPT
  ‚Ä¢ Llama2.c https://github.com/karpathy/llama2.c
  ‚Ä¢ TinyLLaMA https://github.com/eivindbohler/tinyllama

  ‚Ä¢   HautuoGPT
  ‚Ä¢   GPT review
  ‚Ä¢   GPT API
  ‚Ä¢   LLMZoo
  ‚Ä¢   LLMFactory




                                                                      https://github.com/orgs/FreedomIntelligence
Satoshi Nakamura, CUHKSZ, Slide credit: Benyou Wang CSC6203/CIE6201                                 2025/12/1
                                                                                                                61
                                  Summary
   ‚Ä¢ What are the Large Language Models
   ‚Ä¢ How large are they.
   ‚Ä¢ Why LLMs
   ‚Ä¢ Emergent capability
   ‚Ä¢ In-context-learning
   ‚Ä¢ Chain of thoughts
   ‚Ä¢ Zero-shot/few-shot COT training




Satoshi Nakamura, CSC3160     01/12/2025    62
