\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Speech-RAG: A Multimodal Course Assistant with Phi-4-MM and Qwen3-Embedding \\
  \vspace{1em}
}

\author{
  Chaoren Wang (122090513) \\
  \texttt{chaorenwang@link.cuhk.edu.cn} \\
}

\begin{document}

\maketitle

\begin{abstract}
  In the era of large language models (LLMs), students still face challenges in accessing course-specific information efficiently, especially in hands-free scenarios. General-purpose models often hallucinate critical details like classroom locations or exam weights. This report presents ``Speech-RAG'': a course assistant designed for persistent context retention. By integrating Microsoft's Phi-4-MM for multimodal speech understanding and Qwen3-Embedding for specialized retrieval-augmented generation (RAG), our system eliminates the fragmentation between document analysis and voice interaction. Validated against a synthetic audio test set generated by Google Gemini TTS, our system achieves 79\% accuracy on spoken queries compared to 58\% for the baseline, effectively reducing factual errors. This improvement highlights the critical role of retrieval augmentation in mitigating hallucinations for course-specific information. The source code will be publicly available at \url{https://github.com/yuantuo666/CSC3160-speech_rag}.
\end{abstract}

\section{Introduction}

In university settings, students frequently require immediate access to information in hands-free scenarios, asking questions such as ``Where is today's CSC3160 lecture held?'' Our benchmarking reveals that general-purpose large models (No-RAG) often provide confident but incorrect answers---a phenomenon known as hallucination~\cite{ji2023survey}. For instance, a model might incorrectly state ``SDS Office'' instead of the correct ``TB102'' classroom, or misquote the final exam weight as 30\% when it is actually 45\%. Such ``confident errors'' can lead to serious real-world consequences, such as missing a class or misallocating study efforts.

Furthermore, existing tools are often fragmented. While Retrieval-Augmented Generation (RAG) addresses accuracy, mainstream platforms frequently separate ``document analysis'' from ``voice conversation,'' failing to meet the need for immediate, hands-free queries in mobile scenarios.

We propose a course voice assistant with memory capabilities. It features a pre-loaded Persistent Knowledge Base, eliminating the inefficiency of repetitive file uploads, and achieves a seamless fusion of voice interaction with high-precision RAG.

\paragraph{Key Contributions}
This work makes several contributions to the field of multimodal educational technology. First, our system achieves significant \textbf{precision correction}, rectifying over 20\% of factual errors made by general-purpose models on queries related to course information, such as attendance policies and assignment deadlines. Second, we demonstrate a novel \textbf{full-stack integration} of Microsoft's Phi-4-MM\footnote{\url{https://huggingface.co/microsoft/Phi-4-multimodal-instruct}} for speech understanding with vLLM~\cite{kwon2023efficient} for accelerated inference, creating a seamless pipeline that bridges the gap between document analysis and voice interaction. Third, by leveraging \textbf{specialized retrieval} with Qwen3-Embedding\footnote{\url{https://huggingface.co/Qwen/Qwen3-Embedding-0.6B}}~\cite{bai2023qwen}, the system gains a deep understanding of domain-specific terminology, enabling it to accurately answer complex questions like ``Which lectures does Assignment 3 cover?'' by locating precise information rather than guessing. Finally, we establish the system's robustness through \textbf{real-world validation}, using a synthetic dataset generated by Google Gemini TTS (\texttt{gemini-2.5-flash-preview-tts})~\cite{comanici2025gemini25pushingfrontier} to simulate diverse speaker accents and emotional tones, ensuring reliable performance in practical scenarios.

\section{Related Work}

\textbf{Retrieval-Augmented Generation (RAG):} RAG enhances LLMs by retrieving relevant documents to ground responses in factual evidence~\cite{lewis2020retrieval}. While early works focused on open-domain QA, recent advancements have explored integrating RAG with speech processing to create end-to-end spoken question-answering systems. Our work builds on this by applying RAG specifically to the domain of course management with multimodal inputs.

\textbf{Multimodal LLMs:} The landscape of Multimodal LLMs is rapidly evolving. Models like Phi-4-MM~\cite{microsoft2025phi4minitechnicalreportcompact} and Qwen3-Omni~\cite{Qwen3-Omni} have demonstrated impressive capabilities in processing audio natively alongside text. Our system leverages Phi-4-MM's ability to handle speech directly, improving inference speed.

\textbf{Embedding \& TTS:} Effective retrieval relies on high-quality text embeddings. The Massive Text Embedding Benchmark (MTEB)~\cite{muennighoff2023mteb} highlights the performance of models like Qwen3-Embedding in semantic search tasks. On the synthesis side, Neural TTS advances (e.g., F5-TTS~\cite{chen2024f5tts}, CosyVoice2~\cite{du2024cosyvoice2}) allow for the generation of highly naturalistic speech, which we utilize via Gemini TTS~\cite{comanici2025gemini25pushingfrontier} to create a challenging and realistic evaluation dataset.

\section{Approach}

Our system implements a comprehensive pipeline that transforms audio input into accurate, context-aware text responses. The architecture is designed for fast inference and high accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{speech_rag.pdf} 
    \caption{System Architecture of the Speech-RAG Course Assistant. The pipeline integrates multimodal speech understanding with a specialized retrieval module.}
    \label{fig:architecture}
\end{figure}

\subsection{System Pipeline}
To address the challenge of providing accurate, hands-free access to course information, we designed a pipeline that integrates multimodal speech understanding with specialized retrieval-augmented generation. The system architecture transforms raw audio input into precise, context-grounded responses through three coordinated stages: multimodal processing, semantic retrieval, and context-aware generation.

The process begins with the \textbf{Multimodal Understanding} stage. Upon receiving a voice query, the system utilizes \textbf{Phi-4-MM} to process the audio signal. Phi-4-MM generates direct speech interpretation and high-fidelity transcription, preserving the semantic nuance of the student's question.

Following transcription, the \textbf{Retrieval Module} queries in the specific course content. The query is encoded into a dense vector representation using \textbf{Qwen3-Embedding-0.6B} within the \textbf{LangChain}\footnote{\url{https://github.com/langchain-ai/langchain}} framework. This embedding is used to search \textbf{ChromaDB}\footnote{\url{https://github.com/chroma-core/chroma}}, a vector database containing indexed chunks of course materials (PDFs and PPTs). This step ensures that the system identifies the specific lecture segments relevant to the user's inquiry.

Finally, the \textbf{Generation} stage synthesizes the answer. We construct a composite prompt that combines the retrieved course context, and the user's original query. This prompt is processed by the \textbf{vLLM} inference engine to generate a concise, factually accurate text response.

\subsection{Implementation \& Optimization}
The system is built on a Python stack, leveraging LangChain for workflow orchestration, vLLM for high-throughput inference, and ChromaDB for efficient vector storage. To maximize performance and retrieval accuracy, we focused on two key optimizations. First, for the \textbf{embedding model}, we use Qwen3-Embedding-0.6B, which have great semantic capture of specialized course terminology with limited size. Second, to ensure the generation speed, we deploying the model on a remote server accelerated with vLLM and accessed via a Cloudflare Tunnel. This architecture offloads heavy computational tasks from the local device, ensuring a responsive user experience.

\section{Experiments}

\subsection{Experimental Setup}
Our experimental design was structured to ensure a robust and realistic evaluation of the system's performance. The foundation of our experiment was the \textbf{Knowledge Base}, which was constructed by integrating all course-related PDF handouts and PowerPoint slides into a unified vector store. This repository served as the single source of truth for all retrieval operations.

To create a challenging \textbf{Test Set}, we employed \textbf{Gemini Flash} (\texttt{gemini-flash-latest}) to automatically generate a series of multiple-choice questions (MCQs) and their corresponding correct answers based on the course materials. These text-based queries were subsequently synthesized into high-fidelity audio files using \textbf{Gemini TTS} (\texttt{gemini-2.5-flash-preview-tts}) with random speakers to ensure speaker diversity. This process yielded a realistic audio test set that captures variations in student intonation, allowing for a rigorous assessment of the system's performance.

\subsection{Evaluation Metrics}
To quantify the system's effectiveness, we established \textbf{Response Accuracy} as the primary evaluation metric. The accuracy was determined by comparing the system's generated answers against the ground truth (GT) derived from the source materials. This comparison was performed automatically using a large language model xFinder\cite{xfinder} to extract answer and compare it with GT. To isolate the impact of different components, we benchmarked performance across four distinct scenarios: (1) \textbf{No-RAG + Text}: a baseline using pure text queries without RAG, (2) \textbf{No-RAG + Audio}: spoken audio queries without RAG, (3) \textbf{Speech-RAG + Text}: pure text queries enhanced with RAG for ablation, and (4) \textbf{Speech-RAG + Audio}: our proposed method, which combines audio input with RAG. This comparative analysis allowed us to measure the specific contributions of both the speech recognition and retrieval-augmented generation modules.

\begin{table}[H]
  \caption{Performance Comparison across different modalities and retrieval settings (N=100).}
  \label{tab:results}
  \centering
  \begin{tabular}{lccc}
    \toprule
    Method & Modality & Accuracy (\%) & Total Time (s) \\
    \midrule
    No-RAG & Text & 66.00 & 117.19 \\
    No-RAG & Audio & 58.00 & 288.16 \\
    Speech-RAG (Ablation) & Text & 91.00 & 92.01 \\
    \textbf{Speech-RAG (Ours)} & \textbf{Audio} & \textbf{79.00} & 677.58 \\
    \bottomrule
  \end{tabular}
\end{table}

The test results on 100 queries demonstrate that RAG is the critical factor for accuracy in course queries. As shown in Table~\ref{tab:results}, incorporating retrieval mechanisms improved performance by 25 percentage points for text inputs (66\% vs. 91\%) and 21 percentage points for audio inputs (58\% vs. 79\%). While the transition from text to speech input introduces a performance drop due to the complexities of acoustic processing, our end-to-end Speech-RAG system (79\%) still significantly outperforms the baseline text-only model without RAG (66\%). This confirms that even with the noise inherent in speech understanding, grounding the model in specific course materials enables it to correct the majority of hallucinations.

\subsection{Case Studies}
To illustrate the practical impact of our RAG-based approach, we analyzed two representative cases where general-purpose models are prone to failure. The outcomes are presented below.

\vspace{1em}
\noindent
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \fbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{Case 1: Lecture Location}
            \vspace{0.5em}
            
            \textbf{User Query:} \\
            ``What is the location for the Wednesday and Friday lectures?''
            \vspace{1em}
            
            \textbf{No-RAG Response:} \\
            \textcolor{red}{``C: SDS office.''}
            \vspace{1em}
            
            \textbf{Speech-RAG Response:} \\
            \textcolor{green}{``B: TB 102.''}
        \end{minipage}
    }
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
    \centering
    \fbox{
        \begin{minipage}{0.95\linewidth}
            \textbf{Case 2: Grading Scheme}
            \vspace{0.5em}
            
            \textbf{User Query:} \\
            ``What is the max percentage weight of the Final Exam?''
            \vspace{1em}
            
            \textbf{No-RAG Response:} \\
            \textcolor{red}{``A: 30 percent.''}
            \vspace{1em}
            
            \textbf{Speech-RAG Response:} \\
            \textcolor{green}{``C: 45 percent.''}
        \end{minipage}
    }
\end{minipage}
\vspace{1em}

These examples highlight how the RAG mechanism prevents the model from providing plausible but false information by grounding its response in course documents. The system retrieves correct details, demonstrating its ability to handle factual and numerical data, which are common failure points for non-retrieval models.

\section{Conclusion}

This project successfully demonstrates a Speech-RAG Course Assistant that addresses the limitations of general AI models. By combining the multimodal capabilities of Phi-4-MM with the precise retrieval of Qwen3-Embedding, we achieved a robust system capable of understanding spoken queries and providing factually accurate responses based on course materials. The system achieves 79\% accuracy on spoken queries, significantly outperforming the 58\% baseline of general-purpose models. Future work could focus on further improving the first package's speed for real-time conversational speeds and expanding the knowledge base to support multi-course queries simultaneously.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
