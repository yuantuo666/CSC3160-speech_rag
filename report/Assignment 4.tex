\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Speech-RAG: A Multimodal Course Assistant with Phi-4-MM and Qwen3-Embedding \\
  \vspace{1em}
}

\author{
  Chaoren Wang (122090513) \\
  \texttt{chaorenwang@link.cuhk.edu.cn} \\
}

\begin{document}

\maketitle

\section{Introduction}

University students frequently require hands-free access to course logistics. However, our benchmarking shows that general-purpose models often suffer from hallucination~\cite{ji2023survey}, providing confident but incorrect answers regarding venues or grading that can lead to missed classes. Furthermore, existing tools often fragment document analysis and voice interaction, failing to support immediate mobile queries. We propose a course voice assistant that integrates a pre-loaded Persistent Knowledge Base with high-precision RAG, offering a seamless, hands-free solution. The source code is available at \url{https://github.com/yuantuo666/CSC3160-speech_rag}.

\paragraph{Key Contributions}
We advance multimodal course assistant through three contributions. First, our system achieves precision correction, reducing factual errors in course queries by over 20\%. Second, we implement a integration of Phi-4-MM\footnote{\url{https://huggingface.co/microsoft/Phi-4-multimodal-instruct}} (with vLLM~\cite{kwon2023efficient} inference engine) and specialized retrieval via Qwen3-Embedding\footnote{\url{https://huggingface.co/Qwen/Qwen3-Embedding-0.6B}}~\cite{bai2023qwen} to accurately locate domain-specific answers. Finally, we validate using a synthetic dataset from Gemini TTS~\cite{comanici2025gemini25pushingfrontier}, confirming robustness across diverse accents and tones.

\section{Related Work}

\paragraph{Multimodal LLMs} The landscape of Multimodal LLMs is evolving rapidly, with models like Phi-4-MM~\cite{microsoft2025phi4minitechnicalreportcompact} and Qwen3-Omni~\cite{Qwen3-Omni} demonstrating native audio-text processing. Unlike traditional pipelines that separate ASR from textual analysis, these models process speech directly. Our system using Phi-4-MM to reduce transcription errors and improve inference speed for real-time interaction.

\paragraph{Retrieval-Augmented Generation (RAG)} RAG mitigates hallucinations by grounding LLM responses in external factual evidence~\cite{lewis2020retrieval}. However, the effectiveness of RAG relies on semantic retrieval rather than keyword matching. Benchmarks like MTEB~\cite{muennighoff2023mteb} highlight the superiority of retrievers such as Qwen3-Embedding for capturing nuance. We integrate these into a RAG framework for course management, enabling the system to resolve complex, domain-specific queries.

\section{Approach}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{speech_rag.pdf} 
    \caption{System Architecture of the Speech-RAG Course Assistant. The pipeline integrates multimodal speech understanding with a specialized retrieval module.}
    \label{fig:architecture}
\end{figure}

As shown in Figure~\ref{fig:architecture}, we propose a pipeline that transforms raw audio into context-grounded responses. Implementation details are provided in Appendix~\ref{implementation} 

\paragraph{Multimodal Understanding} The process begins by capturing the student's voice query. We utilize \textbf{Phi-4-MM} to process the audio signal directly, generating high-fidelity transcriptions that preserve the semantic nuance of the question, effectively preparing the input for the retrieval chain. 

\paragraph{Retrieval Module} The transcribed query is encoded into a dense vector using \textbf{Qwen3-Embedding} within the \textbf{LangChain}\footnote{\url{https://github.com/langchain-ai/langchain}} framework. This embedding is used to perform a semantic search against \textbf{ChromaDB}\footnote{\url{https://github.com/chroma-core/chroma}}, a vector database containing indexed course materials (PDFs and PPTs), ensuring the retrieval of precise lecture segments relevant to the inquiry.

\paragraph{Generation} Finally, we construct a composite prompt combining the retrieved course context and the user's original query, synthesizing a factually accurate text response.

\section{Experiments}

\paragraph{Setup \& Results}
Our evaluation is built on a knowledge base constructed by indexing all course PDFs and slides into a vector store. We generated a \textbf{Test Set} of multiple-choice questions using \textbf{Gemini} (\texttt{gemini-2.5-flash}). These queries were synthesized into speech using \textbf{Gemini TTS} (\texttt{gemini-2.5-flash-preview-tts}), employing random speaker profiles to simulate realistic acoustic diversity and intonation. 

We utilize \textbf{Accuracy} as the primary metric, verified automatically against ground truth using the xFinder~\cite{xfinder} model. We benchmarked four scenarios to isolate the impact of retrieval and modality: \textbf{No-RAG} vs. \textbf{Speech-RAG} across both \textbf{Text} and \textbf{Audio} inputs.

\begin{table}[H]
    \centering
    \small
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \caption{Comparison across modalities (N=100).}
        \label{tab:results}
        \vspace{2pt}
        
        \resizebox{\linewidth}{!}{
            \begin{tabular}{lcc}
                \toprule
                \textbf{Method} & \textbf{Modality} & \textbf{Acc. (\%)} \\
                \midrule
                No-RAG & Text & 66.00  \\
                No-RAG & Audio & 58.00  \\
                Speech-RAG (Ab.) & Text & 91.00  \\
                \textbf{Speech-RAG} & \textbf{Audio} & \textbf{79.00}  \\
                \bottomrule
            \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Representative correction cases.}
        \label{tab:cases}
        \vspace{2pt}
        
        % Case 1 Box
        \fbox{
            \begin{minipage}{0.95\linewidth}
              \textbf{Case 1: Lecture Location}
              \par\vspace{0.1em}
              \textbf{Query:} ``Location for Wednesday lectures?''
              \par
              \textbf{No-RAG:} \textcolor{red}{``C: SDS office.''}
              \textbf{Ours:} \textcolor{green!60!black}{``B: TB 102.''}
            \end{minipage}
        }
        
        \vspace{0.2em}
        
        % Case 2 Box
        \fbox{
            \begin{minipage}{0.95\linewidth}
              \textbf{Case 2: Grading Scheme}
              \par\vspace{0.1em}
              \textbf{Query:} ``Max percentage weight of Final Exam?''
              \par
              \textbf{No-RAG:} \textcolor{red}{``A: 30 percent.''}
              \textbf{Ours:} \textcolor{green!60!black}{``C: 45 percent.''}
            \end{minipage}
        }
    \end{minipage}
\end{table}

As shown in Table~\ref{tab:results}, retrieval is the critical factor for reliability. Incorporating RAG improves accuracy by 25\% for text and 21\% for audio inputs. Although acoustic processing introduces minor degradation compared to pure text (91\% vs. 79\%), our end-to-end \textbf{Speech-RAG} system significantly outperforms the text-only baseline without RAG (66\%). This confirms that grounding the model in course materials effectively neutralizes hallucinations, even amidst the noise of speech understanding.

\paragraph{Case Studies}
We highlight two cases where general-purpose models fail. As illustrated in Table~\ref{tab:cases}, the No-RAG baseline hallucinates plausible but incorrect logistics. In contrast, our system retrieves specific document chunks (e.g., distinguishing ``TB 102'' from generic office locations).

\section{Conclusion}

We presented a Speech-RAG Course Assistant designed to bridge the gap between hands-free interaction and factual precision in educational settings. By synergizing the multimodal capabilities of Phi-4-MM with the dense retrieval of Qwen3-Embedding, we established a robust pipeline that grounds spoken queries directly in course materials. Our evaluation demonstrates a 79\% accuracy rate on audio inputs, significantly outperforming the 58\% baseline of general-purpose models. Future work will focus on optimizing inference latency for fluid, real-time conversation and scaling the architecture to support multi-course curricula simultaneously.

\bibliographystyle{unsrt}
\bibliography{references}

\appendix

\section{Implementation Details}
\label{implementation}
The system is built on a Python stack, leveraging LangChain for workflow orchestration, vLLM for high-throughput inference, and ChromaDB for efficient vector storage. To maximize performance and retrieval accuracy, we focused on two key optimizations. First, for the \textbf{embedding model}, we use Qwen3-Embedding-0.6B, which have great semantic capture of specialized course terminology with limited size. Second, to ensure the generation speed, we deploying the model on a remote server accelerated with vLLM and accessed via a Cloudflare Tunnel. This architecture offloads heavy computational tasks from the local device, ensuring a responsive user experience.


\end{document}
